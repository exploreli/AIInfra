<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# 05.Attention å˜ç§ç®—æ³•

## 1. ä¼ ç»ŸAttentionç—›ç‚¹

ä¼ ç»Ÿ Attention(MHA)ï¼Œä¸»è¦ä¸»è¦åœ¨æ—¶é—´å¤æ‚åº¦ å’Œæ˜¾å­˜å ç”¨ ä¸¤ä¸ªæ–¹é¢çš„ç“¶é¢ˆ, æœ¬æ–‡ç€é‡è®²è§£**æ˜¾å­˜ç“¶é¢ˆåŠè§£å†³åŠæ³•**, ä¸‹ä¸€ç¯‡åˆ™è®²è§£æ—¶é—´å¤æ‚åº¦çš„é—®é¢˜ã€‚

### æ—¶é—´å¤æ‚åº¦

ä¼ ç»ŸAttentionä¸­ æ—¶é—´å¤æ‚åº¦ä¸º $O(n^2)$ ï¼Œè¿™**é™åˆ¶äº†åºåˆ—é•¿åº¦çš„å¤„ç†**

### æ˜¾å­˜ç“¶é¢ˆ

æ¨¡å‹åœ¨çº¿æ¨ç†ä¸­ ä¸»è¦åŒ…å«å¦‚ä¸‹å†…å®¹:

#### æ¨¡å‹æƒé‡

BF16ç²¾åº¦ä¸‹ï¼Œæ¯ä¸ªå‚æ•°å 2å­—èŠ‚. ä»¥32Bä¸ºä¾‹, åˆ™æƒé‡éƒ¨åˆ†æ˜¾å­˜å ç”¨è®¡ç®—å¦‚ä¸‹:

```math
32 * 10^9 * 2 / 10^9 = 64GB
```
#### æ¿€æ´»å€¼ + æ¡†æ¶

é€å±‚è®¡ç®—ä¸”é‡Šæ”¾, å ç”¨è¾ƒå°, ä¿å®ˆä¼°è®¡çº¦100MB/è¯·æ±‚(å…ˆä¸è®¡å…¥)

#### KVCache

åœ¨æ¨¡å‹éƒ¨ç½², æ¨ç†åœºæ™¯ä¸‹ä¼ ç»Ÿattention, MHAä¸‹, æ˜¾å­˜å ç”¨å¦‚ä¸‹:

```math
memory = 2 * sequence\_length * n\_layers * d\_model * precision\_byte * batch\_size
```

- 2: æŒ‡çš„æ˜¯ key cache å’Œvalue cacheä¸¤ä¸ª
- sequence_length: æŒ‡åºåˆ—é•¿åº¦
- n_layers: æŒ‡ transformer layer(block)çš„å±‚æ•°
- d_model: æŒ‡çš„æ˜¯éšè—å±‚ç»´åº¦, åœ¨MHAåœºæ™¯ä¸‹ = num_heads(å¤´çš„ä¸ªæ•°) * head_dim(æ¯ä¸ªå¤´çš„ç»´åº¦)
- precision_byte: ç²¾åº¦å¯¹åº”å­—èŠ‚æ•°, eg: bf16 å¯¹åº”2å­—èŠ‚,p32 å¯¹åº”4å­—èŠ‚
- batch_size: å¯¹åº”ä¸€æ¬¡æ¨ç†çš„batchæ•°

ä»¥å¸¸è§ä½¿ç”¨åŠå¼€æºé¡¹ç›®(Qwenç³»åˆ—)ä¸­é…ç½®, è®¡ç®—æ˜¾å­˜å ç”¨:
- ç²¾åº¦: bf16
- transformer layers: 64å±‚
- d_model: 5120
- sequence_length: 2048


```math
kvcache ~= 2 * 2048 * 64 * 5120 * 2 * batch\_size / 10^9 GB
```

- kvcache: 

|    batch_size    | 16 |   32   |
| :------: | :------: | :------: |
|  æ¨¡å‹æƒé‡ |   64.0  |   64.0     |
| KV Cache |   42.9  |    85.8     |
|    æ€»è®¡  |  \~=106.9GB | \~= 149.8GB |

å¸¸ç”¨nvidiaæ˜¾å¡æ˜¾å­˜: 4090(24G)ï¼Œ5090(32G),A100(80G), A800(80G), H20(96G), H200(141G),H800(80G)ã€‚

ç»ä¸Šè¿°èƒŒæ™¯æè¿°, æˆ‘ä»¬å·²ç»å¯¹æ¨¡å‹éƒ¨ç½², æ¨ç†æ˜¾å­˜å ç”¨ æœ‰åˆæ­¥æ¦‚å¿µ, ä¸‹é¢å¼€å§‹è®²è§£ä¼˜åŒ–æ€è·¯

## 2. ä¼˜åŒ–æ€è·¯

å‡å°‘KV Cacheçš„ç›®çš„å°±æ˜¯è¦å®ç°åœ¨æ›´å°‘çš„è®¾å¤‡ä¸Šæ¨ç†æ›´é•¿çš„Contextï¼Œæˆ–è€…åœ¨ç›¸åŒçš„Contexté•¿åº¦
ä¸‹è®©æ¨ç†çš„batch sizeæ›´å¤§ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦æˆ–è€…æ›´å¤§çš„ååæ€»é‡ã€‚ 

ä¸‹è¿°æåˆ°çš„MQA, GQA, MLA éƒ½æ˜¯å›´ç»•â€œå¦‚ä½•å‡å°‘KV CacheåŒæ—¶å°½å¯èƒ½åœ°ä¿è¯æ•ˆæœâ€è¿™ä¸ªä¸»é¢˜å‘å±•è€Œæ¥

![MHA vs MQA](./images/AttentionAll.png)


## 1. MQA (Multi-Query Attention) å¤šæŸ¥è¯¢æ³¨æ„åŠ›

### 1.1 ç®€ä»‹

![MHA vs MQA](./images/MQA.png)

Multi-Query Attention (MQA) æ˜¯ä¼ ç»Ÿ Multi-Head Attention çš„ä¸€ç§ä¼˜åŒ–å˜ä½“ï¼Œå®ƒé€šè¿‡åœ¨æ¯ä¸ªheadå…±äº« Key å’Œ Value, åªæœ‰Qåœ¨ä¸åŒheadä¸­ä¸åŒçš„æ–¹å¼æ¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæŸ¥è¯¢çš„å¤šæ ·æ€§ã€‚

- æ–‡ç« æ¥æº: 2019 å¹´ Google è®ºæ–‡
- åŸå§‹æ–‡ç« : https://arxiv.org/pdf/1911.02150

### 1.2 è§£å†³çš„é—®é¢˜

- **å†…å­˜æ•ˆç‡**ï¼šå‡å°‘ KV ç¼“å­˜çš„å†…å­˜å ç”¨
- **è®¡ç®—æ•ˆç‡**ï¼šé™ä½æ³¨æ„åŠ›è®¡ç®—çš„æ—¶é—´å¤æ‚åº¦
- **æ¨ç†åŠ é€Ÿ**ï¼šåœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦
- **èµ„æºä¼˜åŒ–**ï¼šåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘æ¨¡å‹å‚æ•°é‡

### 1.3 æ•°å­¦è¡¨è¾¾

**åœ¨ MQA ä¸­ï¼Œå¤šä¸ª Query å¤´å…±äº«åŒä¸€ä¸ª Key å’Œ Value çŸ©é˜µ**ï¼š

$$
\begin{align*}
\text{MQA}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
\text{å…¶ä¸­}\quad \text{head}_i &= \text{Attention}(Q_i W_i^Q, K W^K, V W^V) \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
\end{align*}
$$

å…¶ä¸­ï¼š
- $Q_i$ï¼šç¬¬ $i$ ä¸ªæŸ¥è¯¢å¤´
- $K, V$ï¼šå…±äº«çš„ Key å’Œ Value çŸ©é˜µ
- $W_i^Q$ï¼šç¬¬ $i$ ä¸ªæŸ¥è¯¢å¤´çš„æƒé‡çŸ©é˜µ
- $W^K, W^V$ï¼šå…±äº«çš„ Key å’Œ Value æƒé‡çŸ©é˜µ

### 1.4 ä¼ªä»£ç å®ç°

```python
def multi_query_attention(X, num_heads, d_model):
    """
    Multi-Query Attention å®ç°
    X: è¾“å…¥åºåˆ— [seq_len, d_model]
    num_heads: æŸ¥è¯¢å¤´æ•°é‡
    d_model: æ¨¡å‹ç»´åº¦
    """
    d_k = d_model // num_heads
    
    # ä¸ºæ¯ä¸ªæŸ¥è¯¢å¤´åˆ›å»º Q çš„æƒé‡çŸ©é˜µ
    W_q = [random_matrix(d_model, d_k) for _ in range(num_heads)]
    
    # å…±äº«çš„ K å’Œ V æƒé‡çŸ©é˜µ
    W_k = random_matrix(d_model, d_k)
    W_v = random_matrix(d_model, d_k)
    
    # è®¡ç®—å…±äº«çš„ K å’Œ V
    K = X @ W_k  # [seq_len, d_k]
    V = X @ W_v  # [seq_len, d_k]
    
    heads = []
    for i in range(num_heads):
        # è®¡ç®—ç¬¬ i ä¸ªæŸ¥è¯¢å¤´
        Q_i = X @ W_q[i]  # [seq_len, d_k]
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = Q_i @ K.T  # [seq_len, seq_len]
        scores = scores / sqrt(d_k)
        
        # åº”ç”¨ softmax
        attention_weights = softmax(scores)
        
        # åŠ æƒæ±‚å’Œ
        head_i = attention_weights @ V
        heads.append(head_i)
    
    # æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
    concat_heads = concatenate(heads, axis=-1)  # [seq_len, d_model]
    
    # æœ€ç»ˆçº¿æ€§å˜æ¢
    W_o = random_matrix(d_model, d_model)
    output = concat_heads @ W_o
    
    return output
```

### 1.5 ä¼˜ç¼ºç‚¹

- ä¼˜ç‚¹: èŠ‚çœæ˜¾å­˜ï¼ŒKV Cache é™ä½ä¸ºåŸå§‹çš„ 1/hï¼Œå‡å°‘è®¡ç®—å’Œé€šä¿¡å¼€é”€ï¼Œæå‡æ¨ç†é€Ÿåº¦ã€‚
- ç¼ºç‚¹: æ€§èƒ½ä¸‹é™ï¼šKV Cache å‹ç¼©è¿‡äºä¸¥é‡ï¼Œå½±å“æ¨¡å‹è®­ç»ƒç¨³å®šæ€§å’Œæ¨¡å‹æ•ˆæœã€‚


## 2. GQA (Grouped-Query Attention) åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›

### 2.1 ç®€ä»‹

![MHA vs MQA vs GQA](./images/GQA.png)

Grouped-Query Attention (GQA) æ˜¯ MQA å’Œä¼ ç»Ÿ Multi-Head Attention ä¹‹é—´çš„æŠ˜ä¸­æ–¹æ¡ˆï¼Œå®ƒå°†æŸ¥è¯¢å¤´åˆ†ç»„ï¼Œæ¯ç»„å…±äº«ä¸€ä¸ª Key å’Œ Value çŸ©é˜µï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

- æ–‡ç« å‡ºå¤„: 2023 Google
- æ–‡ç« é“¾æ¥: https://arxiv.org/pdf/2305.13245

### 2.2 è§£å†³çš„é—®é¢˜

- **å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡**ï¼šåœ¨ MQA å’Œ MHA ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹
- **çµæ´»é…ç½®**ï¼šæ”¯æŒä¸åŒçš„åˆ†ç»„ç­–ç•¥
- **æ¸è¿›ä¼˜åŒ–**ï¼šå¯ä»¥é€æ­¥ä» MHA è¿ç§»åˆ° MQA
- **ä»»åŠ¡é€‚åº”æ€§**ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´åˆ†ç»„æ•°é‡

### 2.3 æ•°å­¦è¡¨è¾¾

**åœ¨ GQA ä¸­ï¼ŒæŸ¥è¯¢å¤´è¢«åˆ†ä¸º $G$ ç»„ï¼Œæ¯ç»„å…±äº« Key å’Œ Value**ï¼š

$$
\begin{align*}
\text{GQA}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
\text{å…¶ä¸­}\quad \text{head}_i &= \text{Attention}(Q_i W_i^Q, K_{g(i)} W_{g(i)}^K, V_{g(i)} W_{g(i)}^V) \\
g(i) &= \lfloor i / (h/G) \rfloor
\end{align*}
$$

å…¶ä¸­ï¼š
- $G$ï¼šåˆ†ç»„æ•°é‡
- $g(i)$ï¼šç¬¬ $i$ ä¸ªæŸ¥è¯¢å¤´æ‰€å±çš„ç»„
- $K_{g(i)}, V_{g(i)}$ï¼šç¬¬ $g(i)$ ç»„å…±äº«çš„ Key å’Œ Value çŸ©é˜µ

### 2.4 ä¼ªä»£ç å®ç°

```python
def grouped_query_attention(X, num_heads, num_groups, d_model):
    """
    Grouped-Query Attention å®ç°
    X: è¾“å…¥åºåˆ— [seq_len, d_model]
    num_heads: æŸ¥è¯¢å¤´æ•°é‡
    num_groups: åˆ†ç»„æ•°é‡
    d_model: æ¨¡å‹ç»´åº¦
    """
    d_k = d_model // num_heads
    heads_per_group = num_heads // num_groups
    
    # ä¸ºæ¯ä¸ªæŸ¥è¯¢å¤´åˆ›å»º Q çš„æƒé‡çŸ©é˜µ
    W_q = [random_matrix(d_model, d_k) for _ in range(num_heads)]
    
    # ä¸ºæ¯ä¸ªç»„åˆ›å»ºå…±äº«çš„ K å’Œ V æƒé‡çŸ©é˜µ
    W_k = [random_matrix(d_model, d_k) for _ in range(num_groups)]
    W_v = [random_matrix(d_model, d_k) for _ in range(num_groups)]
    
    # è®¡ç®—æ¯ç»„çš„ K å’Œ V
    K_groups = []
    V_groups = []
    for g in range(num_groups):
        K_g = X @ W_k[g]  # [seq_len, d_k]
        V_g = X @ W_v[g]  # [seq_len, d_k]
        K_groups.append(K_g)
        V_groups.append(V_g)
    
    heads = []
    for i in range(num_heads):
        # ç¡®å®šå½“å‰å¤´æ‰€å±çš„ç»„
        group_id = i // heads_per_group
        
        # è®¡ç®—ç¬¬ i ä¸ªæŸ¥è¯¢å¤´
        Q_i = X @ W_q[i]  # [seq_len, d_k]
        
        # ä½¿ç”¨å¯¹åº”ç»„çš„ K å’Œ V
        K_g = K_groups[group_id]
        V_g = V_groups[group_id]
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = Q_i @ K_g.T  # [seq_len, seq_len]
        scores = scores / sqrt(d_k)
        
        # åº”ç”¨ softmax
        attention_weights = softmax(scores)
        
        # åŠ æƒæ±‚å’Œ
        head_i = attention_weights @ V_g
        heads.append(head_i)
    
    # æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
    concat_heads = concatenate(heads, axis=-1)  # [seq_len, d_model]
    
    # æœ€ç»ˆçº¿æ€§å˜æ¢
    W_o = random_matrix(d_model, d_model)
    output = concat_heads @ W_o
    
    return output
```

### 2.5 ä¼˜ç¼ºç‚¹

- ä¼˜ç‚¹: 
   - æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å¹³è¡¡ï¼šä¿è¯ KV å¤šæ ·æ€§åŒæ—¶ï¼Œå‡å°‘ KV Cache å¤§å°ï¼›
   - ç¨³å®šæ€§ï¼šç›¸æ¯” MQAï¼Œè®­ç»ƒè¿‡ç¨‹è¾ƒä¸ºç¨³å®š
- ç¼ºç‚¹: 
   - éœ€äººä¸ºåˆç†è®¾ç½®ç»„æ•° g


## 3. MLA (Multi-Latent Attention) å¤šæ½œåœ¨æ³¨æ„åŠ›

### 3.1 ç®€ä»‹

![MLA](./images/MLA.png)

Multi-Latent Attention (MLA) æ˜¯ä¸€ç§åˆ›æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒé€šè¿‡å¼•å…¥æ½œåœ¨å˜é‡æ¥å»ºæ¨¡å¤æ‚çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å’Œå¤æ‚å…³ç³»ã€‚

- æ–‡ç« å‡ºå¤„: 2024.09 Deepseekåœ¨åˆç‰ˆDeepseek V3æ¨¡å‹æ¨å‡ºæ—¶æŠ€æœ¯æŠ¥å‘Š 
- æ–‡ç« é“¾æ¥: https://github.com/LRriver/DeepSeek-V3/blob/main/DeepSeek_V3.pdf

### 3.2 è§£å†³çš„é—®é¢˜

- **é•¿è·ç¦»ä¾èµ–**ï¼šæ›´å¥½åœ°å»ºæ¨¡åºåˆ—ä¸­çš„é•¿è·ç¦»å…³ç³»
- **å¤æ‚æ¨¡å¼æ•æ‰**ï¼šé€šè¿‡æ½œåœ¨å˜é‡æ•æ‰å¤æ‚çš„æ³¨æ„åŠ›æ¨¡å¼
- **è®¡ç®—æ•ˆç‡**ï¼šåœ¨ä¿æŒè¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ä¼˜åŒ–è®¡ç®—å¤æ‚åº¦

### 3.3 è®¡ç®—æ­¥éª¤

#### 1. è®¡ç®—ä»£è¡¨ KV Cacheçš„æ½œåœ¨å‘é‡

![MLAStep1](./images/MLAstep1.png)

$ğ‘_ğ‘¡^ğ¾ğ‘‰$Â æ˜¯åœ¨æ—¶é—´æ­¥Â ğ‘¡Â è®¡ç®—çš„é”®å€¼ç¼“å­˜æ½œåœ¨å‘é‡ã€‚Â $ğ‘Š^ğ·ğ¾ğ‘‰$Â æ˜¯ä¸€ä¸ªæƒé‡çŸ©é˜µï¼Œç”¨äºå°†éšè—çŠ¶æ€Â $â„ğ‘¡$ æ˜ å°„åˆ°é”®å€¼ç¼“å­˜ç©ºé—´ï¼Œè¿™ä¸€æ­¥å¯ä»¥é€šè¿‡ç¥ç»ç½‘ç»œæ˜ å°„å¾—åˆ°ã€‚$ğ‘_ğ‘¡^ğ¾ğ‘‰$ ç›¸å¯¹äºåŸæ¥çš„ $â„ğ‘¡$ Â è¦å°å¾ˆå¤šã€‚

#### 2. è®¡ç®— Query, Key å’Œ value æ½œåœ¨å‘é‡

![MLAStep2](./images/MLAstep2.png)

$ğ‘˜_ğ‘¡^ğ¶$Â æ˜¯ Key æ½œåœ¨å‘é‡ï¼Œé€šè¿‡å°†Â  $ğ‘_ğ‘¡^ğ¾ğ‘‰$ ä¸æƒé‡çŸ©é˜µÂ $ğ‘Š^ğ‘ˆğ¾$Â ç›¸ä¹˜å¾—åˆ°ï¼Œè¿™ä¸€æ­¥æ˜¯åšä¸Šé‡‡æ ·ï¼Œé€šè¿‡æ½œå‘é‡ç‰¹å¾ $ğ‘_ğ‘¡^ğ¾ğ‘‰$ æ˜ å°„å¾—åˆ°è¾ƒå¤§çš„ $ğ‘˜_ğ‘¡^ğ¶$ ç”¨äºåç»­çš„æ³¨æ„åŠ›è®¡ç®—ã€‚ $ğ‘£_ğ‘¡^ğ¶$ è®¡ç®—åŒç†ã€‚

![MLAStep5](./images/MLAstep5.png)

K å‘é‡çš„è®¡ç®—ç±»ä¼¼ï¼Œé€šè¿‡æ½œåœ¨å‘é‡è®¡ç®—å¾—åˆ°å‚ä¸åç»­ MHA è®¡ç®—çš„æŸ¥è¯¢å‘é‡ q

#### 3. è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰

![MLAStep3](./images/MLAstep3.png)

ç”¨äºåœ¨é”®å‘é‡ä¸­å¼•å…¥ä½ç½®ä¿¡æ¯

#### 4. ç»„åˆæ½œå‘é‡kå’Œä½ç½®ç¼–ç kå¾—åˆ°æœ€ç»ˆçš„é”®å‘é‡

![MLAStep4](./images/MLAstep4.png)

æœ€ç»ˆçš„é”®å‘é‡Â $ğ‘˜_(ğ‘¡,ğ‘–)$ æ˜¯é€šè¿‡å°†å†…å®¹ç›¸å…³çš„é”®å‘é‡ $ğ‘˜_(ğ‘¡,ğ‘–)^ğ¶$ å’Œä½ç½®ç¼–ç Â $ğ‘˜_ğ‘¡^ğ‘…$Â è¿æ¥èµ·æ¥å¾—åˆ°

#### 5. æ³¨æ„åŠ›è®¡ç®—

![MLAStep5](./images/MLAstep6.png)

æœ€ç»ˆçš„æ³¨æ„åŠ›è¾“å‡ºÂ ğ‘¢_ğ‘¡Â æ˜¯é€šè¿‡å°†æŸ¥è¯¢Â (ğ‘_(ğ‘¡,ğ‘–)) ï¼Œé”®Â  (ğ‘˜_(ğ‘¡,ğ‘–)) Â å’Œå€¼Â  (ğ‘£_(ğ‘—,ğ‘–)^ğ¶) Â ç»“åˆèµ·æ¥è®¡ç®—ã€‚å…¶ä¸­ ğ‘œ_(ğ‘¡,ğ‘–) æ˜¯ç¬¬Â ğ‘–Â ä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡º

### 3.4 æ•°å­¦è¡¨è¾¾

MLA é€šè¿‡æ½œåœ¨å˜é‡ $Z$ æ¥å»ºæ¨¡æ³¨æ„åŠ›åˆ†å¸ƒï¼š

$$
\begin{align*}
\text{MLA}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
\text{å…¶ä¸­}\quad \text{head}_i &= \text{Attention}(Q_i, K_i, V_i, Z_i) \\
\text{Attention}(Q, K, V, Z) &= \text{softmax}\left(\frac{Q K^T + Z}{\sqrt{d_k}}\right) V \\
Z &= \text{LatentModule}(Q, K)
\end{align*}
$$

å…¶ä¸­ï¼š
- $Z$ï¼šæ½œåœ¨å˜é‡çŸ©é˜µï¼Œç”¨äºå»ºæ¨¡å¤æ‚çš„æ³¨æ„åŠ›æ¨¡å¼
- $\text{LatentModule}$ï¼šæ½œåœ¨å˜é‡ç”Ÿæˆæ¨¡å—
- å…¶ä»–ç¬¦å·å«ä¹‰ä¸æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶ç›¸åŒ

### 3.5 ä¼ªä»£ç å®ç°

```python
def multi_latent_attention(X, num_heads, d_model, latent_kv_dim, rope_params):
    """
    Multi-Latent Attention å®ç°ï¼ˆæŒ‰ç…§â€œè®¡ç®—æ­¥éª¤ 1-5â€å¯¹åº”å®ç°ï¼‰
    X: è¾“å…¥åºåˆ— [seq_len, d_model]
    num_heads: æ³¨æ„åŠ›å¤´æ•°é‡
    d_model: æ¨¡å‹ç»´åº¦
    latent_kv_dim: KV ç¼“å­˜æ½œåœ¨å‘é‡ç»´åº¦ï¼ˆæ­¥éª¤1ï¼šç¼©å°åçš„ç»´åº¦ï¼‰
    rope_params: RoPE ä½ç½®ç¼–ç å‚æ•°ï¼ˆæ­¥éª¤3ï¼‰
    """
    d_k = d_model // num_heads

    # æ­¥éª¤ 1ï¼šè®¡ç®—ä»£è¡¨ KV Cache çš„æ½œåœ¨å‘é‡ c_t^KV
    # å°†éšè—çŠ¶æ€ h_tï¼ˆæ­¤å¤„ä¸º X çš„æ¯ä¸ªæ—¶é—´æ­¥è¡Œå‘é‡ï¼‰æ˜ å°„åˆ°æ›´å°çš„ KV ç©ºé—´
    W_d_kv = random_matrix(d_model, latent_kv_dim)
    C_kv = X @ W_d_kv  # [seq_len, latent_kv_dim]

    # æ­¥éª¤ 2ï¼šç”± c_t^KV ä¸Šé‡‡æ ·å¾—åˆ°å†…å®¹ç›¸å…³çš„æ½œåœ¨å‘é‡ Q^C, K^C, V^C
    # ä½¿ç”¨ä¸åŒçš„ä¸Šé‡‡æ ·çŸ©é˜µåˆ†åˆ«å¾—åˆ°æ¯ä¸ª head çš„ Q/K/V å†…å®¹åˆ†é‡
    W_uq = [random_matrix(latent_kv_dim, d_k) for _ in range(num_heads)]
    W_uk = [random_matrix(latent_kv_dim, d_k) for _ in range(num_heads)]
    W_uv = [random_matrix(latent_kv_dim, d_k) for _ in range(num_heads)]

    Q_c_list = []  # æ¯ä¸ª head çš„ Q^C
    K_c_list = []  # æ¯ä¸ª head çš„ K^C
    V_c_list = []  # æ¯ä¸ª head çš„ V^C
    for i in range(num_heads):
        Q_c_list.append(C_kv @ W_uq[i])  # [seq_len, d_k]
        K_c_list.append(C_kv @ W_uk[i])  # [seq_len, d_k]
        V_c_list.append(C_kv @ W_uv[i])  # [seq_len, d_k]

    # æ­¥éª¤ 3ï¼šè®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œå¾—åˆ°ä½ç½®ç›¸å…³çš„é”®å‘é‡ K^R
    # è¿™é‡Œç”Ÿæˆå½¢çŠ¶ä¸º [seq_len, d_k] çš„ä½ç½®ç¼–ç åˆ†é‡ï¼Œä¾›å„ head å…±äº«
    K_r = apply_rope_positions(C_kv.shape[0], d_k, rope_params)  # [seq_len, d_k]

    # æ­¥éª¤ 4ï¼šç»„åˆæ½œå‘é‡ k^C ä¸ä½ç½®ç¼–ç  k^R å¾—åˆ°æœ€ç»ˆé”®å‘é‡ K
    # æ–‡æ¡£æè¿°ä¸ºâ€œè¿æ¥ï¼ˆconcatï¼‰â€ï¼ŒéšåæŠ•å½±å› d_kï¼Œä¿è¯ä¸ Q^C ç»´åº¦ä¸€è‡´
    W_k_mix = [random_matrix(2 * d_k, d_k) for _ in range(num_heads)]
    K_list = []  # æ¯ä¸ª head çš„æœ€ç»ˆ K
    for i in range(num_heads):
        K_concat = concatenate([K_c_list[i], K_r], axis=-1)  # [seq_len, 2*d_k]
        K_i = K_concat @ W_k_mix[i]  # [seq_len, d_k]
        K_list.append(K_i)

    # æ­¥éª¤ 5ï¼šæ³¨æ„åŠ›è®¡ç®—ï¼ˆä½¿ç”¨ Q=Q^C, K=ç»„åˆåçš„ K, V=V^Cï¼‰
    heads = []
    for i in range(num_heads):
        scores = Q_c_list[i] @ K_list[i].T  # [seq_len, seq_len]
        scores = scores / sqrt(d_k)
        attention_weights = softmax(scores)
        head_i = attention_weights @ V_c_list[i]
        heads.append(head_i)

    # æ‹¼æ¥æ‰€æœ‰å¤´è¾“å‡ºå¹¶çº¿æ€§å˜æ¢
    concat_heads = concatenate(heads, axis=-1)  # [seq_len, d_model]
    W_o = random_matrix(d_model, d_model)
    output = concat_heads @ W_o
    return output

def apply_rope_positions(seq_len, d_k, rope_params):
    """
    æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ç”Ÿæˆï¼ˆå¯¹åº”â€œæ­¥éª¤ 3â€ï¼‰
    è¿”å›å½¢çŠ¶ [seq_len, d_k] çš„ä½ç½®ç¼–ç å‘é‡ï¼Œç”¨äºå½¢æˆ K^R
    """
    # ä¼ªä»£ç ï¼šæ ¹æ® rope_params äº§ç”Ÿ cos/sin å‚æ•°ï¼Œå¹¶ç”Ÿæˆå¯¹åº”ç»´åº¦çš„ä½ç½®å‘é‡
    return rope_matrix(seq_len, d_k, rope_params)  # [seq_len, d_k]

def rope_matrix(seq_len, d_k, rope_params):
    """
    æ„å»º RoPE åŸºå‘é‡çŸ©é˜µï¼ˆç¤ºæ„ï¼›ç»†èŠ‚å®ç°å–å†³äºå…·ä½“ RoPE å®šä¹‰ï¼‰
    """
    # å ä½å®ç°ï¼šè¿”å›ä¸€ä¸ªä¸ç»´åº¦åŒ¹é…çš„å ä½çŸ©é˜µ
    return random_matrix(seq_len, d_k)
```


## 4. ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”

| ç‰¹æ€§ | MQA | GQA | MLA |
|------|-----|-----|-----|
| **KV ç¼“å­˜å ç”¨** | æ˜¾å­˜å ç”¨ä½(ä»…éœ€ 1 ç»„ KV ç¼“å­˜) | æ˜¾å­˜å ç”¨ä½äº MHA, ä½†é«˜äº MQA(åˆ†ç»„å…±äº« KV cache) | æ˜¾å­˜å ç”¨æ˜¾è‘—é™ä½(ä½ç§©å‹ç¼©) |
|**è®¡ç®—å¤æ‚åº¦**|æœ€ä½(å…±äº« KV è®¡ç®—)|ä¸­ç­‰(åˆ†ç»„å…±äº« KV è®¡ç®—)|ä½äº MHA å’Œ GQA(ä½ç§©ç©ºé—´è®¡ç®—)|
|**æ¨¡å‹æ•ˆæœ**|ç•¥ä½äº MHA(å…±äº« KV å¯¼è‡´ä¿¡æ¯æŸå¤±)|æ¥è¿‘ MHA(åˆ†ç»„å…±äº«å¹³è¡¡æ€§èƒ½æ•ˆç‡)|æ¥è¿‘ MHA(ä½ç§©å‹ç¼©ä¿ç•™å…³é”®ç‰¹å¾)|
|**åº”ç”¨æ¨¡å‹**|Falcon ç³»åˆ—æ¨¡å‹|LLaMA-2/LLaMA-3ã€Qwen3|DeepSeek-V3ã€Kimi-K2|


## 5.æ€»ç»“ä¸æ€è€ƒ

æœ¬ç« èŠ‚ä¸º å¯¹ä¼ ç»Ÿattentionæœºåˆ¶, æ˜¾å­˜å ç”¨é—®é¢˜çš„ä¼˜åŒ–æ”¹è¿›ã€‚ä½†ä¹Ÿä»…æ˜¯åŸºç¡€, å„å®¶åœ¨è§£å†³é•¿åºåˆ—é—®é¢˜æ—¶, è¿˜ä¼šæœ‰å¾ˆå¤šå…¶ä»–çš„è§£å†³åŠæ³•, å…¶ä¸­ä¸å°‘éƒ½æ˜¯ä»¥ä¸Šè¿°attentionå˜ç§ä¸ºåŸºç¡€ã€‚æœªæ¥attentionä¼˜åŒ–çš„æ–¹å‘ä¸€å®šä¸º**é«˜æ•ˆ, å¯æ‰©å±•, æ³¨æ„åŠ›æ•ˆæœå¥½, é€‚åˆé•¿ä¸Šä¸‹æ–‡çš„æ–¹å‘** å¦‚ä¸‹:

- å‡å°‘å¤æ‚åº¦ï¼šéšç€å¤§æ¨¡å‹å‘å±•ï¼Œé€šè¿‡ä¼˜åŒ– Attention è®¡ç®—å¤æ‚åº¦æå‡º Linear Attention ç­‰
- é•¿åºåˆ—å»ºæ¨¡â€‹â€‹ï¼šç»“åˆç¨€ç–æ³¨æ„åŠ›ä¸åŠ¨æ€è·¯ç”±ï¼Œè¿›ä¸€æ­¥å‹ç¼©KV Cacheã€‚
- å¤šæ¨¡æ€æ‰©å±•â€‹â€‹ï¼šæ¢ç´¢è·¨æ¨¡æ€æ³¨æ„åŠ›äº¤äº’ï¼Œå¦‚è§†è§‰-è¯­è¨€è”åˆè¡¨å¾ã€‚

## æœ¬èŠ‚è§†é¢‘

<html>
<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114682388093947&bvid=BV1GzMUz8Eav&cid=30498491822&p=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>

## å‚è€ƒä¸å¼•ç”¨

!!!!!!!!!åŠ å…¥å‚è€ƒçš„æ–‡ç« å’Œå†…å®¹