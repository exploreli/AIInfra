<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# CODE 01: ZeRO æ˜¾å­˜ä¼˜åŒ–å®è·µ

> Author by: è®¸ç¿å²·

ç›®å‰**GPU + Pytorch + Megatron + DeepSpeed**æ˜¯å¸¸ç”¨çš„è®­ç»ƒè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ã€‚è€Œå¾®è½¯å¼€å‘çš„**DeepSpeed**çš„æ ¸å¿ƒå°±æ˜¯**ZeRO**(Zero Redundancy Optimizer)ï¼Œå®ƒæ˜¯ä¸€ç§æ˜¾å­˜ä¼˜åŒ–çš„**æ•°æ®å¹¶è¡Œ**(data parallelismï¼ŒDP)æ–¹æ¡ˆã€‚**ZeRO**æŠ€æœ¯é€šè¿‡æ¶ˆé™¤**æ•°æ®å¹¶è¡Œ**ä¸­çš„æ˜¾å­˜å†—ä½™ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå¤§æ¨¡å‹æ‰€éœ€çš„æ˜¾å­˜ã€‚

æœ¬å®éªŒå°†æ·±å…¥æ¢è®¨ ZeRO çš„å„çº§ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å®é™…ä»£ç æ¼”ç¤ºå’Œåˆ†æï¼Œç†è§£ä¸åŒçº§åˆ«çš„ ZeRO å¦‚ä½•å®ç°æ˜¾å­˜ä¼˜åŒ–ã€‚

ğŸ“Œ **PS**ï¼šæœ¬ Notebook **ä»…ç”¨äºæ•™å­¦ç›®çš„**ï¼Œæ‰€æœ‰ ZeRO å®ç°å‡ä¸º**å• GPU ä¸Šçš„ç®€åŒ–æ¨¡æ‹Ÿ**ï¼Œ**å¹¶æœªä½¿ç”¨çœŸå®å¤š GPU å¹¶è¡Œæˆ–é€šä¿¡åŸè¯­**ï¼ˆå¦‚ `all_reduce`, `reduce_scatter`ï¼‰ã€‚çœŸå® ZeRO éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆå¦‚ DeepSpeed + å¤š GPUï¼‰ï¼Œå…¶æ˜¾å­˜èŠ‚çœæ•ˆæœåœ¨ **N ä¸ª GPU æ—¶æ‰ä½“ç°ä¸º 1/N**ã€‚æœ¬å®éªŒé€šè¿‡â€œäººä¸ºåˆ†ç‰‡ + æ‰‹åŠ¨é‡Šæ”¾â€æ¥**æ¨¡æ‹Ÿ**åˆ†ç‰‡è¡Œä¸ºï¼Œå¸®åŠ©ç†è§£æ ¸å¿ƒæ€æƒ³ã€‚


## 1. æ¨¡å‹æ˜¾å­˜å ç”¨åˆ†æ

åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œæ˜¾å­˜å ç”¨å¯ä»¥åˆ†ä¸º**Activation**å’Œ**Model State**ä¸¤éƒ¨åˆ†ï¼š

**Activation**ï¼š
- **ä¸­é—´æ¿€æ´»å€¼**ï¼ˆActivationsï¼‰ï¼šåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚ä¼šäº§ç”Ÿä¸­é—´æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼éœ€è¦å†åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚
- **è¾“å…¥æ•°æ®**ï¼ˆInputs Dataï¼‰ï¼šæ‰¹å¤„ç†ä¸­è¾“å…¥æ•°æ®ä¹Ÿå ç”¨æ˜¾å­˜ï¼Œå°¤å…¶æ˜¯å½“æ‰¹å¤„ç†è¾ƒå¤§æ—¶ã€‚

**Model State**ï¼š

- **ä¼˜åŒ–å™¨çŠ¶æ€**ï¼ˆOptimizer Statesï¼‰ï¼šæ˜¯Optimizer åœ¨è¿›è¡Œæ¢¯åº¦æ›´æ–°æ—¶æ‰€éœ€è¦ç”¨åˆ°æ•°æ®ã€‚ä¸€äº›ä¼˜åŒ–å™¨(å¦‚Adam)éœ€è¦å­˜å‚¨é¢å¤–çš„çŠ¶æ€ä¿¡æ¯ï¼Œå¦‚æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼å’Œå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼ã€‚ä¾‹å¦‚SGDä¸­çš„Momentum,å³ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶çš„**Float32 Master Parameters**ã€‚
- **æ¨¡å‹å‚æ•°**ï¼ˆParametersï¼‰ï¼šæ¨¡å‹çš„å¯å­¦ä¹ æƒé‡ï¼Œå¦‚å­˜å‚¨åœ¨æ˜¾å­˜ä¸­çš„æ¨¡å‹æƒé‡å’Œåç½®é¡¹ã€‚
- **æ¢¯åº¦**ï¼ˆGradientsï¼‰ï¼šåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚å…¶å†³å®šäº†å‚æ•°çš„æ›´æ–°æ–¹å‘ã€‚

å®ƒä»¬ä¸‰ä¸ªç®€ç§°**OPG**ï¼Œå…¶ä¸­**ä¼˜åŒ–å™¨çŠ¶æ€**ä¼šå æ®å¤§çº¦2å€å‚æ•°é‡çš„æ˜¾å­˜ç©ºé—´ï¼Œè¿™å–å†³äºé€‰æ‹©çš„ä¼˜åŒ–å™¨ï¼Œä¹Ÿæ˜¯æ•´ä¸ªè®­ç»ƒä¸­å æ®æœ€å¤§ç©ºé—´çš„éƒ¨åˆ†ã€‚

**ZeRO**åˆ™åœ¨æ•°æ®å¹¶è¡Œçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†å¯¹å†—ä½™**Model States**çš„ä¼˜åŒ–ã€‚ä½¿ç”¨ZeROåï¼Œå„ä¸ªè¿›ç¨‹ä¹‹ååªä¿å­˜å®Œæ•´çŠ¶æ€çš„**1/GPUs**ï¼Œäº’ä¸é‡å ï¼Œä¸å†å­˜åœ¨å†—ä½™ã€‚ç›¸æ¯”ä¼ ç»Ÿæ•°æ®å¹¶è¡Œçš„ç®€å•å¤åˆ¶ï¼Œ**ZeRO**é€šè¿‡å°†æ¨¡å‹çš„**å‚æ•°**ã€**æ¢¯åº¦** å’Œ **ä¼˜åŒ–å™¨çŠ¶æ€**åˆ’åˆ†åˆ°ä¸åŒçš„è¿›ç¨‹æ¥æ¶ˆé™¤å†—ä½™çš„å†…å­˜å ç”¨ï¼Œä¹Ÿå°±å¼•å‡º**ZeRO**çš„ä¸‰ä¸ªä¸åŒçš„çº§åˆ«,åˆ†åˆ«å¯¹åº”**Model States**ä¸åŒç¨‹åº¦çš„åˆ†å‰²(Partition)ï¼š

- **ZeRO-1**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ã€‚
- **ZeRO-2**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ä¸**æ¢¯åº¦**ã€‚
- **ZeRO-3**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ã€**æ¢¯åº¦**ä¸**å‚æ•°**ã€‚

![](./images/Code01ZeRO00.png)

å¯¹äºä½¿ç”¨ Adam ä¼˜åŒ–å™¨çš„æ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨å¯ä¼°ç®—ä¸ºï¼š

```text
æ€»æ˜¾å­˜ = å‚æ•°æ˜¾å­˜ + æ¢¯åº¦æ˜¾å­˜ + ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ + æ¿€æ´»å€¼æ˜¾å­˜
å‚æ•°æ˜¾å­˜ = å‚æ•°é‡ Ã— 4 å­—èŠ‚ï¼ˆFP32ï¼‰
æ¢¯åº¦æ˜¾å­˜ = å‚æ•°é‡ Ã— 4 å­—èŠ‚ï¼ˆFP32ï¼‰
ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ = å‚æ•°é‡ Ã— 16 å­—èŠ‚ï¼ˆFP32 Adamï¼‰
```

æ˜¾å­˜å ç”¨åˆ†æå·¥å…·

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import defaultdict

class MemoryAnalyzer:
    """ç®€åŒ–çš„æ˜¾å­˜åˆ†æå·¥å…·ç±»"""

    def __init__(self):
        self.memory_stats = defaultdict(list)
        self.previous_allocated = 0

    def record(self, tag=''):
        """è®°å½•å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3    # GB
        delta = allocated - self.previous_allocated
        self.previous_allocated = allocated

        self.memory_stats['allocated'].append(allocated)
        self.memory_stats['reserved'].append(reserved)
        self.memory_stats['delta'].append(delta)

        print(f"{tag}: å·²åˆ†é…: {allocated:.2f}GB, å˜åŒ–: {delta:+.2f}GB")
        return allocated

# åˆ›å»ºæµ‹è¯•æ¨¡å‹
def create_model(hidden_size=2048, num_layers=8):
    """åˆ›å»ºç®€åŒ–çš„ Transformer é£æ ¼æ¨¡å‹"""
    layers = []
    for _ in range(num_layers):
        layers.append(nn.Linear(hidden_size, hidden_size))
        layers.append(nn.ReLU())
    return nn.Sequential(*layers)

# æ‰§è¡Œæ˜¾å­˜åˆ†æ
def analyze_memory():
    # ç¡®ä¿ä½¿ç”¨ GPU
    if not torch.cuda.is_available():
        print("CUDA ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ˜¾å­˜åˆ†æ")
        return

    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    # è®°å½•åˆå§‹çŠ¶æ€
    analyzer.record("åˆå§‹çŠ¶æ€")

    # åˆ›å»ºæ¨¡å‹
    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    analyzer.record("ä¼˜åŒ–å™¨åˆ›å»ºå")

    # æ¨¡æ‹Ÿæ•°æ®
    inputs = torch.randn(32, 2048).cuda()
    targets = torch.randn(32, 2048).cuda()
    analyzer.record("æ•°æ®åŠ è½½å")

    # å‰å‘ä¼ æ’­
    outputs = model(inputs)
    loss = F.mse_loss(outputs, targets)
    analyzer.record("å‰å‘ä¼ æ’­å")

    # åå‘ä¼ æ’­
    loss.backward()
    analyzer.record("åå‘ä¼ æ’­å")

    # ä¼˜åŒ–å™¨æ­¥éª¤
    optimizer.step()
    analyzer.record("ä¼˜åŒ–å™¨æ›´æ–°å")

    return analyzer.memory_stats

# æ‰§è¡Œåˆ†æ
memory_stats = analyze_memory()
```

é€šè¿‡è¿™ä¸ªåˆ†æå·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°åœ¨æ¯ä¸ªè®­ç»ƒé˜¶æ®µæ˜¾å­˜çš„ä½¿ç”¨æƒ…å†µå˜åŒ–ã€‚åœ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¿™äº›æ˜¾å­˜å ç”¨ä¼šæˆå€å¢é•¿ï¼Œå‡¸æ˜¾äº† ZeRO ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚

```
åˆå§‹çŠ¶æ€: å·²åˆ†é…: 0.00GB, å˜åŒ–: +0.00GB
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB
ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
æ•°æ®åŠ è½½å: å·²åˆ†é…: 0.15GB, å˜åŒ–: +0.02GB
å‰å‘ä¼ æ’­å: å·²åˆ†é…: 0.27GB, å˜åŒ–: +0.12GB
åå‘ä¼ æ’­å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.12GB
ä¼˜åŒ–å™¨æ›´æ–°å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.00GB
```

## 2. ZeRO-1: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡

ZeRO-1 é€šè¿‡å°†ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°å¤šä¸ª GPU ä¸Šæ¥å‡å°‘æ˜¾å­˜å ç”¨ã€‚åœ¨ä¼ ç»Ÿæ•°æ®å¹¶è¡Œä¸­ï¼Œæ¯ä¸ª GPU éƒ½ä¿å­˜å®Œæ•´çš„ä¼˜åŒ–å™¨çŠ¶æ€å‰¯æœ¬ï¼Œè¿™é€ æˆäº†å¤§é‡çš„æ˜¾å­˜å†—ä½™ã€‚

ZeRO-1 çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ¯ä¸ª GPU åªä¿å­˜ä¸€éƒ¨åˆ†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå½“éœ€è¦æ›´æ–°å‚æ•°æ—¶ï¼Œé€šè¿‡é›†åˆé€šä¿¡æ“ä½œè·å–å®Œæ•´çš„æ¢¯åº¦ä¿¡æ¯ã€‚

æ•°å­¦è¡¨è¾¾ä¸Šï¼Œå¯¹äº Adam ä¼˜åŒ–å™¨ï¼Œæ¯ä¸ª GPU åŸæœ¬éœ€è¦å­˜å‚¨ï¼š

- å‚æ•°ï¼š$Î˜$
- æ¢¯åº¦ï¼š$âˆ‡Î˜$
- åŠ¨é‡ï¼š$m$
- æ–¹å·®ï¼š$v$

ZeRO-1 åˆ†ç‰‡åï¼Œæ¯ä¸ª GPU åªå­˜å‚¨ï¼š

- å®Œæ•´å‚æ•°ï¼š$Î˜$
- å®Œæ•´æ¢¯åº¦ï¼š$âˆ‡Î˜$
- 1/N çš„åŠ¨é‡ï¼š$m_i$
- 1/N çš„æ–¹å·®ï¼š$v_i$

å…¶ä¸­ N æ˜¯ GPU æ•°é‡ã€‚

![](./images/Code01ZeRO01.png)

ZeRO-1 ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡

```python
class Zero1Optimizer:
    """ç®€åŒ–çš„ ZeRO-1 ä¼˜åŒ–å™¨å®ç°"""

    def __init__(self, params, optimizer_class=torch.optim.Adam, shard_size=4, **kwargs):
        self.params = list(params)
        self.shard_size = shard_size
        self.shards = self._create_shards()

        # ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizers = [optimizer_class(shard,** kwargs) for shard in self.shards]

    def _create_shards(self):
        """å°†å‚æ•°åˆ†æˆå¤šä¸ªåˆ†ç‰‡"""
        shards = []
        for i in range(0, len(self.params), self.shard_size):
            shards.append(self.params[i:i+self.shard_size])
        return shards

    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()

    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œåªæ›´æ–°åˆ†ç‰‡å‚æ•°"""
        for optimizer in self.optimizers:
            optimizer.step()

# æµ‹è¯• ZeRO-1 æ•ˆæœ
def test_zero1():
    if not torch.cuda.is_available():
        return

    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    # ä½¿ç”¨ ZeRO-1 ä¼˜åŒ–å™¨
    optimizer = Zero1Optimizer(model.parameters(), shard_size=4, lr=1e-3)
    analyzer.record("ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå")

    # ç®€å•è®­ç»ƒæ­¥éª¤
    inputs = torch.randn(32, 2048).cuda()
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()

    analyzer.record("è®­ç»ƒä¸€æ­¥å")
    return analyzer.memory_stats

# æ‰§è¡Œæµ‹è¯•
zero1_stats = test_zero1()
```

è¿™ä¸ªç®€åŒ–å®ç°å±•ç¤ºäº† ZeRO-1 çš„æ ¸å¿ƒæ€æƒ³ï¼šæ¯ä¸ª GPU åªå­˜å‚¨å’Œæ›´æ–°ä¸€éƒ¨åˆ†å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œé€šè¿‡é€šä¿¡æ“ä½œç¡®ä¿æ‰€æœ‰ GPU çš„å‚æ•°ä¿æŒä¸€è‡´ã€‚

```
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB
ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.26GB
```

## 3. ZeRO-2: æ¢¯åº¦åˆ†ç‰‡

ZeRO-2 åœ¨ ZeRO-1 çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¸ä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè¿˜åˆ†ç‰‡æ¢¯åº¦ã€‚è¿™è¿›ä¸€æ­¥å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œå› ä¸ºæ¢¯åº¦é€šå¸¸ä¸å‚æ•°å¤§å°ç›¸åŒã€‚

![](./images/Code01ZeRO02.png)

åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª GPU è®¡ç®—å…¶åˆ†é…åˆ°çš„å‚æ•°çš„æ¢¯åº¦ï¼Œç„¶åé€šè¿‡ Reduce-Scatter æ“ä½œèšåˆæ¢¯åº¦ã€‚è¿™æ ·æ¯ä¸ª GPU åªä¿å­˜ä¸€éƒ¨åˆ†æ¢¯åº¦ï¼Œè€Œä¸æ˜¯å…¨éƒ¨æ¢¯åº¦ã€‚æ¢¯åº¦åˆ†ç‰‡çš„æ•°å­¦è¡¨è¾¾ï¼š

- ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ª GPU å­˜å‚¨å®Œæ•´æ¢¯åº¦ $âˆ‡Î˜$
- ZeRO-2ï¼šæ¯ä¸ª GPU å­˜å‚¨ 1/N çš„æ¢¯åº¦ $âˆ‡Î˜_i$


```python
class Zero2Optimizer(Zero1Optimizer):
    """ç®€åŒ–çš„ ZeRO-2 ä¼˜åŒ–å™¨å®ç°ï¼Œåœ¨ ZeRO-1 åŸºç¡€ä¸Šå¢åŠ æ¢¯åº¦åˆ†ç‰‡"""

    def __init__(self, params, optimizer_class=torch.optim.Adam, shard_size=4, **kwargs):
        super().__init__(params, optimizer_class, shard_size,** kwargs)
        self.grad_shards = self._create_shards()  # æ¢¯åº¦åˆ†ç‰‡ä¸å‚æ•°åˆ†ç‰‡å¯¹åº”

    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œåªå¤„ç†åˆ†ç‰‡æ¢¯åº¦"""
        # æ¨¡æ‹Ÿæ¢¯åº¦åˆ†ç‰‡èšåˆ
        for i, shard in enumerate(self.grad_shards):
            # åªèšåˆå½“å‰åˆ†ç‰‡éœ€è¦çš„æ¢¯åº¦
            for param in shard:
                if param.grad is not None:
                    # æ¨¡æ‹Ÿåˆ†å¸ƒå¼æ¢¯åº¦èšåˆ
                    param.grad = param.grad.contiguous()

            # æ›´æ–°å½“å‰åˆ†ç‰‡
            self.optimizers[i].step()

# æµ‹è¯• ZeRO-2 æ•ˆæœ
def test_zero2():
    if not torch.cuda.is_available():
        return

    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    # ä½¿ç”¨ ZeRO-2 ä¼˜åŒ–å™¨
    optimizer = Zero2Optimizer(model.parameters(), shard_size=4, lr=1e-3)
    analyzer.record("ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå")

    # ç®€å•è®­ç»ƒæ­¥éª¤
    inputs = torch.randn(32, 2048).cuda()
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()

    analyzer.record("è®­ç»ƒä¸€æ­¥å")
    return analyzer.memory_stats

# æ‰§è¡Œæµ‹è¯•
zero2_stats = test_zero2()
```

ZeRO-2 é€šè¿‡æ¢¯åº¦åˆ†ç‰‡è¿›ä¸€æ­¥å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œä½†å¢åŠ äº†é€šä¿¡å¼€é”€ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®ç½‘ç»œå¸¦å®½å’Œè®¡ç®—èƒ½åŠ›æƒè¡¡è¿™ç§æƒè¡¡ã€‚

```
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB
ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.31GB, å˜åŒ–: +0.18GB
```

## 4. ZeRO-3: å‚æ•°åˆ†ç‰‡

ZeRO-3 æ˜¯ ZeRO ç³»åˆ—çš„æœ€ç»ˆå½¢æ€ï¼Œå®ƒä¸ä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè¿˜åˆ†ç‰‡æ¨¡å‹å‚æ•°æœ¬èº«ã€‚è¿™æ„å‘³ç€æ¯ä¸ª GPU åªå­˜å‚¨æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¤§å¤§é™ä½äº†å•ä¸ª GPU çš„æ˜¾å­˜éœ€æ±‚ã€‚

![](./images/Code01ZeRO03.png)

ZeRO-3 çš„å·¥ä½œåŸç†ï¼š

1. å‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ª GPU åªè®¡ç®—å®ƒæ‹¥æœ‰çš„å‚æ•°éƒ¨åˆ†
2. éœ€è¦å…¶ä»– GPU çš„å‚æ•°æ—¶ï¼Œé€šè¿‡é€šä¿¡æ“ä½œè·å–
3. åå‘ä¼ æ’­æ—¶ç±»ä¼¼ï¼Œåªè®¡ç®—æœ¬åœ°å‚æ•°çš„æ¢¯åº¦
4. é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é€šä¿¡æ¨¡å¼æœ€å°åŒ–é€šä¿¡å¼€é”€

å‚æ•°åˆ†ç‰‡çš„æ•°å­¦è¡¨è¾¾ï¼š

- ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ª GPU å­˜å‚¨å®Œæ•´å‚æ•° $Î˜$
- ZeRO-3ï¼šæ¯ä¸ª GPU å­˜å‚¨ 1/N çš„å‚æ•° $Î˜_i$

```python
class Zero3Model(nn.Module):
    """ç®€åŒ–çš„ ZeRO-3 å‚æ•°åˆ†ç‰‡æ¨¡å‹"""

    def __init__(self, base_model, shard_id=0, num_shards=4):
        super().__init__()
        self.shard_id = shard_id
        self.num_shards = num_shards
        self.layers = nn.ModuleList()

        # åˆ†ç‰‡æ¨¡å‹å±‚
        total_layers = len(base_model)
        layers_per_shard = (total_layers + num_shards - 1) // num_shards
        start = shard_id * layers_per_shard
        end = min(start + layers_per_shard, total_layers)

        # åªä¿ç•™å½“å‰åˆ†ç‰‡è´Ÿè´£çš„å±‚
        for i in range(start, end):
            self.layers.append(base_model[i])

    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼Œåªè®¡ç®—å½“å‰åˆ†ç‰‡"""
        for layer in self.layers:
            x = layer(x)
        return x

# æµ‹è¯• ZeRO-3 æ•ˆæœ
def test_zero3():
    if not torch.cuda.is_available():
        return

    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    base_model = create_model()
    # åˆ›å»ºåˆ†ç‰‡æ¨¡å‹ï¼ˆåªåŠ è½½ 1/4 çš„å‚æ•°ï¼‰
    model = Zero3Model(base_model, shard_id=0, num_shards=4).cuda()
    analyzer.record("ZeRO-3 æ¨¡å‹åˆ›å»ºå")

    # ä¼˜åŒ–å™¨åªéœ€è¦ä¼˜åŒ–éƒ¨åˆ†å‚æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    analyzer.record("ä¼˜åŒ–å™¨åˆ›å»ºå")

    # ç®€å•è®­ç»ƒæ­¥éª¤
    inputs = torch.randn(32, 2048).cuda()
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()

    analyzer.record("è®­ç»ƒä¸€æ­¥å")
    return analyzer.memory_stats

# æ‰§è¡Œæµ‹è¯•
zero3_stats = test_zero3()
```

ZeRO-3 æä¾›äº†æœ€å¤§çš„æ˜¾å­˜èŠ‚çœï¼Œä½†é€šä¿¡å¼€é”€ä¹Ÿæœ€å¤§ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸éœ€è¦ç»“åˆå„ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚é€šä¿¡è®¡ç®—é‡å ã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼Œæ¥å¹³è¡¡æ˜¾å­˜èŠ‚çœå’Œè®­ç»ƒé€Ÿåº¦ã€‚

```
ZeRO-3 æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.03GB, å˜åŒ–: +0.03GB
ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.03GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.11GB, å˜åŒ–: +0.08GB
```

## 5. Zero Offload æŠ€æœ¯

Zero Offload æŠ€æœ¯å°†ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°å¸è½½åˆ° CPU å†…å­˜æˆ– NVMe å­˜å‚¨ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†å¯è®­ç»ƒçš„æ¨¡å‹è§„æ¨¡ã€‚è¿™ç§æŠ€æœ¯ç‰¹åˆ«é€‚åˆåœ¨æœ‰é™ GPU å†…å­˜ç¯å¢ƒä¸‹è®­ç»ƒè¶…å¤§æ¨¡å‹ã€‚

![](./images/Code01ZeRO04.png)

Offload çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ CPU å†…å­˜å’Œ NVMe å­˜å‚¨ä½œä¸º GPU æ˜¾å­˜çš„æ‰©å±•ï¼Œé€šè¿‡å¼‚æ­¥æ•°æ®ä¼ è¾“å’Œè®¡ç®—é‡å æ¥æœ€å°åŒ–æ€§èƒ½å½±å“ã€‚

```python
class CPUOffloadOptimizer:
    """ç®€åŒ–çš„ CPU Offload ä¼˜åŒ–å™¨"""

    def __init__(self, params, optimizer_class=torch.optim.Adam, **kwargs):
        self.params = list(params)

        # åœ¨ CPU ä¸Šåˆ›å»ºå‚æ•°å‰¯æœ¬å’Œä¼˜åŒ–å™¨
        self.cpu_params = [p.detach().cpu().requires_grad_(False) for p in self.params]
        self.optimizer = optimizer_class(self.cpu_params,** kwargs)

    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œä½¿ç”¨ CPU è®¡ç®—"""
        # å°†æ¢¯åº¦å¤åˆ¶åˆ° CPU
        for gpu_param, cpu_param in zip(self.params, self.cpu_params):
            if gpu_param.grad is not None:
                cpu_param.grad = gpu_param.grad.cpu()

        # åœ¨ CPU ä¸Šæ›´æ–°
        self.optimizer.step()

        # å°†æ›´æ–°åçš„å‚æ•°å¤åˆ¶å› GPU
        for gpu_param, cpu_param in zip(self.params, self.cpu_params):
            gpu_param.data.copy_(cpu_param.data)

# æµ‹è¯• CPU Offload æ•ˆæœ
def test_cpu_offload():
    if not torch.cuda.is_available():
        return

    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    # ä½¿ç”¨ CPU Offload ä¼˜åŒ–å™¨
    optimizer = CPUOffloadOptimizer(model.parameters(), lr=1e-3)
    analyzer.record("CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå")

    # ç®€å•è®­ç»ƒæ­¥éª¤
    inputs = torch.randn(32, 2048).cuda()
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()

    analyzer.record("è®­ç»ƒä¸€æ­¥å")
    return analyzer.memory_stats

# æ‰§è¡Œæµ‹è¯•
offload_stats = test_cpu_offload()
```

```
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB
CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.25GB, å˜åŒ–: +0.12GB
```

## 6. æ€§èƒ½åˆ†æä¸å®éªŒç»“æœ

ä¸ºäº†éªŒè¯ ZeRO å„çº§åˆ«çš„æ•ˆæœï¼Œæˆ‘ä»¬è®¾è®¡äº†ä»¥ä¸‹å®éªŒï¼š

```python
# æ±‡æ€»æ‰€æœ‰æ–¹æ³•çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
def compare_methods():
    if not torch.cuda.is_available():
        return

    print("\n æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):")
    print("-" * 40)

    # é‡æ–°è¿è¡ŒåŸºç¡€æµ‹è¯•
    baseline = analyze_memory()
    zero1 = test_zero1()
    zero2 = test_zero2()
    zero3 = test_zero3()
    offload = test_cpu_offload()

    # æå–æœ€ç»ˆæ˜¾å­˜ä½¿ç”¨é‡
    print(f"åŸºç¡€æ–¹æ³•: {baseline['allocated'][-1]:.2f}GB")
    print(f"ZeRO-1: {zero1['allocated'][-1]:.2f}GB ({(1-zero1['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")
    print(f"ZeRO-2: {zero2['allocated'][-1]:.2f}GB ({(1-zero2['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")
    print(f"ZeRO-3: {zero3['allocated'][-1]:.2f}GB ({(1-zero3['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")
    print(f"CPU Offload: {offload['allocated'][-1]:.2f}GB ({(1-offload['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")

# æ‰§è¡Œå¯¹æ¯”
compare_methods()
```

é€šè¿‡è¿™ä¸ªå®éªŒï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ° ZeRO å„çº§åˆ«å¯¹æ˜¾å­˜å ç”¨çš„ä¼˜åŒ–æ•ˆæœã€‚åœ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¿™äº›ä¼˜åŒ–å¯ä»¥å¸¦æ¥æ•°å€ç”šè‡³æ•°åå€çš„æ˜¾å­˜èŠ‚çœã€‚

```
æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):
----------------------------------------
åŸºç¡€æ–¹æ³•: 0.39GB
ZeRO-1: 0.39GB (0.0% èŠ‚çœ)
ZeRO-2: 0.31GB (20.5% èŠ‚çœ)
ZeRO-3: 0.11GB (71.8% èŠ‚çœ)
CPU Offload: 0.25GB (35.9% èŠ‚çœ)
```

## æ€»ç»“ä¸æ€è€ƒ

ZeRO æŠ€æœ¯é€šè¿‡åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†å¤§æ¨¡å‹è®­ç»ƒçš„æ˜¾å­˜éœ€æ±‚ã€‚æœ¬å®éªŒé€šè¿‡ä»£ç å®ç°å’ŒåŸç†åˆ†æï¼Œæ·±å…¥æ¢è®¨äº†ï¼š

1. **ZeRO-1**ï¼šä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼Œå‡å°‘çº¦ 4 å€æ˜¾å­˜å ç”¨
2. **ZeRO-2**ï¼šæ¢¯åº¦åˆ†ç‰‡ï¼Œè¿›ä¸€æ­¥å‡å°‘çº¦ 8 å€æ˜¾å­˜å ç”¨
3. **ZeRO-3**ï¼šå‚æ•°åˆ†ç‰‡ï¼Œæœ€å¤§å¯å‡å°‘çº¦ N å€æ˜¾å­˜å ç”¨ï¼ˆN ä¸º GPU æ•°é‡ï¼‰
4. **Zero Offload**ï¼šå°†æ•°æ®å¸è½½åˆ° CPU/NVMeï¼Œæ”¯æŒè®­ç»ƒè¶…å¤§æ¨¡å‹

è¿™äº›æŠ€æœ¯å¯ä»¥ç»„åˆä½¿ç”¨ï¼Œæ ¹æ®å…·ä½“çš„ç¡¬ä»¶ç¯å¢ƒå’Œæ¨¡å‹å¤§å°é€‰æ‹©æœ€åˆé€‚çš„é…ç½®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒDeepSpeed æ¡†æ¶æä¾›äº†å®Œæ•´çš„ ZeRO å®ç°ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ç»è¿‡ä¼˜åŒ–çš„å®˜æ–¹å®ç°ã€‚
