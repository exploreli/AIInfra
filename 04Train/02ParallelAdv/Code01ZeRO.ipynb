{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed035d7",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 01: ZeRO 显存优化实践\n",
    "\n",
    "> Author by: 许灿岷\n",
    "\n",
    "目前**GPU + Pytorch + Megatron + DeepSpeed**是常用的训练超大规模语言模型的训练框架。而微软开发的**DeepSpeed**的核心就是**ZeRO**(Zero Redundancy Optimizer)，它是一种显存优化的**数据并行**(data parallelism，DP)方案。**ZeRO**技术通过消除**数据并行**中的显存冗余，显著降低了训练大模型所需的显存。\n",
    "\n",
    "本实验将深入探讨 ZeRO 的各级优化技术，通过实际代码演示和分析，理解不同级别的 ZeRO 如何实现显存优化。\n",
    "\n",
    "📌 **PS**：本 Notebook **仅用于教学目的**，所有 ZeRO 实现均为**单 GPU 上的简化模拟**，**并未使用真实多 GPU 并行或通信原语**（如 `all_reduce`, `reduce_scatter`）。真实 ZeRO 需要分布式训练环境（如 DeepSpeed + 多 GPU），其显存节省效果在 **N 个 GPU 时才体现为 1/N**。本实验通过“人为分片 + 手动释放”来**模拟**分片行为，帮助理解核心思想。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b015c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 模型显存占用分析\n",
    "\n",
    "在深度学习训练中，显存占用可以分为**Activation**和**Model State**两部分：\n",
    "\n",
    "**Activation**：\n",
    "- **中间激活值**（Activations）：在前向传播过程中，神经网络的每一层会产生中间激活值，这些激活值需要再反向传播过程中用来计算梯度。\n",
    "- **输入数据**（Inputs Data）：批处理中输入数据也占用显存，尤其是当批处理较大时。\n",
    "\n",
    "**Model State**：\n",
    "\n",
    "- **优化器状态**（Optimizer States）：是Optimizer 在进行梯度更新时所需要用到数据。一些优化器(如Adam)需要存储额外的状态信息，如梯度的移动平均值和平方梯度的移动平均值。例如SGD中的Momentum,即使用混合精度训练时的**Float32 Master Parameters**。\n",
    "- **模型参数**（Parameters）：模型的可学习权重，如存储在显存中的模型权重和偏置项。\n",
    "- **梯度**（Gradients）：在反向传播过程中计算得到的梯度，用于更新模型参数。其决定了参数的更新方向。\n",
    "\n",
    "它们三个简称**OPG**，其中**优化器状态**会占据大约2倍参数量的显存空间，这取决于选择的优化器，也是整个训练中占据最大空间的部分。\n",
    "\n",
    "**ZeRO**则在数据并行的基础上，引入了对冗余**Model States**的优化。使用ZeRO后，各个进程之后只保存完整状态的**1/GPUs**，互不重叠，不再存在冗余。相比传统数据并行的简单复制，**ZeRO**通过将模型的**参数**、**梯度** 和 **优化器状态**划分到不同的进程来消除冗余的内存占用，也就引出**ZeRO**的三个不同的级别,分别对应**Model States**不同程度的分割(Partition)：\n",
    "\n",
    "- **ZeRO-1**： 分割**优化器状态**。\n",
    "- **ZeRO-2**： 分割**优化器状态**与**梯度**。\n",
    "- **ZeRO-3**： 分割**优化器状态**、**梯度**与**参数**。\n",
    "\n",
    "![](./images/Code01ZeRO00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a250f",
   "metadata": {},
   "source": [
    "\n",
    "对于使用 Adam 优化器的模型，显存占用可估算为：\n",
    "```\n",
    "总显存 = 参数显存 + 梯度显存 + 优化器状态显存 + 激活值显存\n",
    "参数显存 = 参数量 × 4 字节（FP32）\n",
    "梯度显存 = 参数量 × 4 字节（FP32）\n",
    "优化器状态显存 = 参数量 × 16 字节（FP32 Adam）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97271d95",
   "metadata": {},
   "source": [
    "显存占用分析工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e400b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始状态: 已分配: 0.00GB, 变化: +0.00GB\n",
      "模型创建后: 已分配: 0.19GB, 变化: +0.19GB\n",
      "优化器创建后: 已分配: 0.19GB, 变化: +0.00GB\n",
      "数据加载后: 已分配: 0.19GB, 变化: +0.00GB\n",
      "前向传播后: 已分配: 0.20GB, 变化: +0.01GB\n",
      "反向传播后: 已分配: 0.39GB, 变化: +0.19GB\n",
      "优化器更新后: 已分配: 0.77GB, 变化: +0.38GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "class MemoryAnalyzer:\n",
    "    \"\"\"简化的显存分析工具类（仅用于教学演示）\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory_stats = defaultdict(list)\n",
    "        self.previous_allocated = 0\n",
    "\n",
    "    def record(self, tag=''):\n",
    "        \"\"\"记录当前 GPU 显存使用情况（单位：GB）\"\"\"\n",
    "        torch.cuda.synchronize()  # 确保所有操作完成\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        delta = allocated - self.previous_allocated\n",
    "        self.previous_allocated = allocated\n",
    "\n",
    "        self.memory_stats['allocated'].append(allocated)\n",
    "        self.memory_stats['reserved'].append(reserved)\n",
    "        self.memory_stats['delta'].append(delta)\n",
    "\n",
    "        print(f\"{tag}: 已分配: {allocated:.2f}GB, 变化: {delta:+.2f}GB\")\n",
    "        return allocated\n",
    "\n",
    "# 创建测试模型\n",
    "def create_model(hidden_size=2048, num_layers=12):\n",
    "    \"\"\"创建一个简单的全连接模型用于测试\"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_layers):\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# 执行显存分析\n",
    "def analyze_memory(seed=42):\n",
    "    \"\"\"执行基础训练流程并记录显存变化\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA 不可用\")\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    analyzer.record(\"初始状态\")\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"模型创建后\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    analyzer.record(\"优化器创建后\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    targets = torch.randn(32, 2048, device='cuda')\n",
    "    analyzer.record(\"数据加载后\")\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, targets)\n",
    "    analyzer.record(\"前向传播后\")\n",
    "\n",
    "    loss.backward()\n",
    "    analyzer.record(\"反向传播后\")\n",
    "\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"优化器更新后\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# 执行分析\n",
    "memory_stats = analyze_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d37206",
   "metadata": {},
   "source": [
    "通过这个分析工具，我们可以清楚地看到在每个训练阶段显存的使用情况变化。在实际的大模型训练中，这些显存占用会成倍增长，凸显了 ZeRO 优化的必要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdae504",
   "metadata": {},
   "source": [
    "```\n",
    "初始状态: 已分配: 0.00GB, 变化: +0.00GB\n",
    "模型创建后: 已分配: 0.13GB, 变化: +0.13GB\n",
    "优化器创建后: 已分配: 0.13GB, 变化: +0.00GB\n",
    "数据加载后: 已分配: 0.13GB, 变化: +0.00GB\n",
    "前向传播后: 已分配: 0.14GB, 变化: +0.01GB\n",
    "反向传播后: 已分配: 0.27GB, 变化: +0.13GB\n",
    "优化器更新后: 已分配: 0.52GB, 变化: +0.25GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff08427",
   "metadata": {},
   "source": [
    "## 2. ZeRO-1: 优化器状态分片\n",
    "\n",
    "ZeRO-1 通过将优化器状态分片到多个 GPU 上来减少显存占用。在传统数据并行中，每个 GPU 都保存完整的优化器状态副本，这造成了大量的显存冗余。\n",
    "\n",
    "ZeRO-1 的核心思想是：每个 GPU 只保存一部分优化器状态，当需要更新参数时，通过集合通信操作获取完整的梯度信息。\n",
    "\n",
    "数学表达上，对于 Adam 优化器，每个 GPU 原本需要存储：\n",
    "\n",
    "- 参数：$Θ$\n",
    "- 梯度：$∇Θ$\n",
    "- 动量：$m$\n",
    "- 方差：$v$\n",
    "\n",
    "ZeRO-1 分片后，每个 GPU 只存储：\n",
    "\n",
    "- 完整参数：$Θ$\n",
    "- 完整梯度：$∇Θ$\n",
    "- 1/N 的动量：$m_i$\n",
    "- 1/N 的方差：$v_i$\n",
    "\n",
    "其中 N 是 GPU 数量。\n",
    "\n",
    "![](./images/Code01ZeRO01.png)\n",
    "\n",
    "ZeRO-1 优化器状态分片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebac3fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型创建后: 已分配: 0.20GB, 变化: +0.20GB\n",
      "ZeRO-1 优化器创建后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.77GB, 变化: +0.56GB\n"
     ]
    }
   ],
   "source": [
    "class Zero1Optimizer:\n",
    "    \"\"\"ZeRO-1 模拟实现：将参数分片，每个分片独立优化器（单 GPU 模拟）\"\"\"\n",
    "\n",
    "    def __init__(self, params, optimizer_class=torch.optim.Adam, num_shards=4, **kwargs):\n",
    "        self.params = list(params)\n",
    "        self.num_shards = num_shards\n",
    "\n",
    "        # 将参数均匀分片\n",
    "        self.shards = []\n",
    "        shard_size = (len(self.params) + num_shards - 1) // num_shards\n",
    "        for i in range(0, len(self.params), shard_size):\n",
    "            self.shards.append(self.params[i:i + shard_size])\n",
    "\n",
    "        # 为每个分片创建独立优化器\n",
    "        self.optimizers = [\n",
    "            optimizer_class(shard, **kwargs) for shard in self.shards\n",
    "        ]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        # 每个分片独立更新（模拟多 GPU 各自更新自己的分片）\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "\n",
    "# 测试 ZeRO-1 效果\n",
    "def test_zero1(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"模型创建后\")\n",
    "\n",
    "    optimizer = Zero1Optimizer(model.parameters(), num_shards=4, lr=1e-3)\n",
    "    analyzer.record(\"ZeRO-1 优化器创建后\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"训练一步后\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# 执行测试\n",
    "zero1_stats = test_zero1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea50a4",
   "metadata": {},
   "source": [
    "这个简化实现展示了 ZeRO-1 的核心思想：每个 GPU 只存储和更新一部分参数的优化器状态，通过通信操作确保所有 GPU 的参数保持一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fabfd7",
   "metadata": {},
   "source": [
    "```\n",
    "模型创建后: 已分配: 0.14GB, 变化: +0.14GB\n",
    "ZeRO-1 优化器创建后: 已分配: 0.14GB, 变化: +0.00GB\n",
    "训练一步后: 已分配: 0.52GB, 变化: +0.38GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8870f",
   "metadata": {},
   "source": [
    "## 3. ZeRO-2: 梯度分片\n",
    "\n",
    "ZeRO-2 在 ZeRO-1 的基础上进一步优化，不仅分片优化器状态，还分片梯度。这进一步减少了显存占用，因为梯度通常与参数大小相同。\n",
    "\n",
    "![](./images/Code01ZeRO02.png)\n",
    "\n",
    "在反向传播过程中，每个 GPU 计算其分配到的参数的梯度，然后通过 Reduce-Scatter 操作聚合梯度。这样每个 GPU 只保存一部分梯度，而不是全部梯度。梯度分片的数学表达：\n",
    "\n",
    "- 传统方法：每个 GPU 存储完整梯度 $∇Θ$\n",
    "- ZeRO-2：每个 GPU 存储 1/N 的梯度 $∇Θ_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410532f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型创建后: 已分配: 0.20GB, 变化: +0.20GB\n",
      "ZeRO-2 优化器创建后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.34GB, 变化: +0.14GB\n"
     ]
    }
   ],
   "source": [
    "class Zero2Optimizer(Zero1Optimizer):\n",
    "    \"\"\"ZeRO-2 模拟：在 ZeRO-1 基础上，只保留当前分片的梯度\"\"\"\n",
    "\n",
    "    def step(self):\n",
    "        current_shard_idx = 0  # 假设当前 GPU 负责第 0 个分片\n",
    "\n",
    "        # 删除非本分片的梯度（模拟 reduce-scatter 后释放）\n",
    "        for shard_idx, shard in enumerate(self.shards):\n",
    "            for param in shard:\n",
    "                if param.grad is not None:\n",
    "                    if shard_idx != current_shard_idx:\n",
    "                        param.grad = None  # ✅ 释放梯度显存\n",
    "\n",
    "        # 仅更新本分片\n",
    "        self.optimizers[current_shard_idx].step()\n",
    "\n",
    "# 测试 ZeRO-2 效果\n",
    "def test_zero2(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"模型创建后\")\n",
    "\n",
    "    optimizer = Zero2Optimizer(model.parameters(), num_shards=4, lr=1e-3)\n",
    "    analyzer.record(\"ZeRO-2 优化器创建后\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"训练一步后\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "zero2_stats = test_zero2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6a0f9",
   "metadata": {},
   "source": [
    "ZeRO-2 通过梯度分片进一步减少了显存占用，但增加了通信开销。在实际应用中，需要根据网络带宽和计算能力权衡这种权衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08975588",
   "metadata": {},
   "source": [
    "```\n",
    "模型创建后: 已分配: 0.13GB, 变化: +0.13GB\n",
    "ZeRO-2 优化器创建后: 已分配: 0.13GB, 变化: +0.00GB\n",
    "训练一步后: 已分配: 0.31GB, 变化: +0.18GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40d456",
   "metadata": {},
   "source": [
    "## 4. ZeRO-3: 参数分片\n",
    "\n",
    "ZeRO-3 是 ZeRO 系列的最终形态，它不仅分片优化器状态和梯度，还分片模型参数本身。这意味着每个 GPU 只存储模型的一小部分参数，大大降低了单个 GPU 的显存需求。\n",
    "\n",
    "![](./images/Code01ZeRO03.png)\n",
    "\n",
    "ZeRO-3 的工作原理：\n",
    "\n",
    "1. 前向传播时，每个 GPU 只计算它拥有的参数部分\n",
    "2. 需要其他 GPU 的参数时，通过通信操作获取\n",
    "3. 反向传播时类似，只计算本地参数的梯度\n",
    "4. 通过精心设计的通信模式最小化通信开销\n",
    "\n",
    "参数分片的数学表达：\n",
    "\n",
    "- 传统方法：每个 GPU 存储完整参数 $Θ$\n",
    "- ZeRO-3：每个 GPU 存储 1/N 的参数 $Θ_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa67c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeRO-3 模型创建后: 已分配: 0.06GB, 变化: +0.06GB\n",
      "优化器创建后: 已分配: 0.06GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.20GB, 变化: +0.14GB\n"
     ]
    }
   ],
   "source": [
    "class Zero3Model(nn.Module):\n",
    "    \"\"\"ZeRO-3 模拟：仅加载模型的一部分层（参数分片）\"\"\"\n",
    "\n",
    "    def __init__(self, base_model, shard_id=0, num_shards=4):\n",
    "        super().__init__()\n",
    "        self.shard_id = shard_id\n",
    "        self.num_shards = num_shards\n",
    "\n",
    "        # 计算当前分片负责的层范围\n",
    "        total_layers = len(base_model)\n",
    "        layers_per_shard = (total_layers + num_shards - 1) // num_shards\n",
    "        start = shard_id * layers_per_shard\n",
    "        end = min(start + layers_per_shard, total_layers)\n",
    "\n",
    "        # 仅保留本分片的层\n",
    "        self.layers = nn.ModuleList([base_model[i] for i in range(start, end)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# 测试 ZeRO-3 效果\n",
    "def test_zero3(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    base_model = create_model()\n",
    "    model = Zero3Model(base_model, shard_id=0, num_shards=4).cuda()\n",
    "    analyzer.record(\"ZeRO-3 模型创建后\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    analyzer.record(\"优化器创建后\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"训练一步后\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# 执行测试\n",
    "zero3_stats = test_zero3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc174396",
   "metadata": {},
   "source": [
    "ZeRO-3 提供了最大的显存节省，但通信开销也最大。在实际应用中，通常需要结合各种优化技术，如通信计算重叠、梯度累积等，来平衡显存节省和训练速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb53782",
   "metadata": {},
   "source": [
    "```\n",
    "ZeRO-3 模型创建后: 已分配: 0.03GB, 变化: +0.03GB\n",
    "优化器创建后: 已分配: 0.03GB, 变化: +0.00GB\n",
    "训练一步后: 已分配: 0.11GB, 变化: +0.08GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b4978",
   "metadata": {},
   "source": [
    "## 5. Zero Offload 技术\n",
    "\n",
    "Zero Offload 技术将优化器状态、梯度和参数卸载到 CPU 内存或 NVMe 存储，进一步扩展了可训练的模型规模。这种技术特别适合在有限 GPU 内存环境下训练超大模型。\n",
    "\n",
    "![](./images/Code01ZeRO04.png)\n",
    "\n",
    "Offload 的核心思想是利用 CPU 内存和 NVMe 存储作为 GPU 显存的扩展，通过异步数据传输和计算重叠来最小化性能影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c355a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型创建后: 已分配: 0.20GB, 变化: +0.20GB\n",
      "CPU Offload 优化器创建后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.39GB, 变化: +0.19GB\n"
     ]
    }
   ],
   "source": [
    "class CPUOffloadOptimizer:\n",
    "    \"\"\"CPU Offload 模拟：优化器状态存储在 CPU\"\"\"\n",
    "\n",
    "    def __init__(self, params, optimizer_class=torch.optim.Adam, **kwargs):\n",
    "        self.gpu_params = list(params)\n",
    "        # 在 CPU 上创建参数副本（无梯度）\n",
    "        self.cpu_params = [p.detach().cpu().clone() for p in self.gpu_params]\n",
    "        self.optimizer = optimizer_class(self.cpu_params, **kwargs)\n",
    "\n",
    "    def step(self):\n",
    "        # 梯度从 GPU → CPU\n",
    "        for gpu_p, cpu_p in zip(self.gpu_params, self.cpu_params):\n",
    "            if gpu_p.grad is not None:\n",
    "                cpu_p.grad = gpu_p.grad.cpu()\n",
    "\n",
    "        # 在 CPU 上更新\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 参数从 CPU → GPU\n",
    "        for gpu_p, cpu_p in zip(self.gpu_params, self.cpu_params):\n",
    "            gpu_p.data.copy_(cpu_p.data)\n",
    "\n",
    "        # 清理 CPU 梯度\n",
    "        for cpu_p in self.cpu_params:\n",
    "            cpu_p.grad = None\n",
    "\n",
    "# 测试 CPU Offload 效果\n",
    "def test_cpu_offload(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"模型创建后\")\n",
    "\n",
    "    optimizer = CPUOffloadOptimizer(model.parameters(), lr=1e-3)\n",
    "    analyzer.record(\"CPU Offload 优化器创建后\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"训练一步后\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# 执行测试\n",
    "offload_stats = test_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae4922",
   "metadata": {},
   "source": [
    "```\n",
    "模型创建后: 已分配: 0.13GB, 变化: +0.13GB\n",
    "CPU Offload 优化器创建后: 已分配: 0.13GB, 变化: +0.00GB\n",
    "训练一步后: 已分配: 0.25GB, 变化: +0.12GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacbd31",
   "metadata": {},
   "source": [
    "## 6. 性能分析与实验结果\n",
    "\n",
    "为了验证 ZeRO 各级别的效果，我们设计了以下实验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe058aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 显存使用对比 (单位: GB):\n",
      "----------------------------------------\n",
      "初始状态: 已分配: 0.02GB, 变化: +0.02GB\n",
      "模型创建后: 已分配: 0.20GB, 变化: +0.19GB\n",
      "优化器创建后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "数据加载后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "前向传播后: 已分配: 0.21GB, 变化: +0.00GB\n",
      "反向传播后: 已分配: 0.39GB, 变化: +0.18GB\n",
      "优化器更新后: 已分配: 0.77GB, 变化: +0.38GB\n",
      "----------------------------------------\n",
      "模型创建后: 已分配: 0.20GB, 变化: +0.20GB\n",
      "ZeRO-1 优化器创建后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.77GB, 变化: +0.56GB\n",
      "----------------------------------------\n",
      "模型创建后: 已分配: 0.20GB, 变化: +0.20GB\n",
      "ZeRO-2 优化器创建后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.34GB, 变化: +0.14GB\n",
      "----------------------------------------\n",
      "ZeRO-3 模型创建后: 已分配: 0.06GB, 变化: +0.06GB\n",
      "优化器创建后: 已分配: 0.06GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.20GB, 变化: +0.14GB\n",
      "----------------------------------------\n",
      "模型创建后: 已分配: 0.20GB, 变化: +0.20GB\n",
      "CPU Offload 优化器创建后: 已分配: 0.20GB, 变化: +0.00GB\n",
      "训练一步后: 已分配: 0.39GB, 变化: +0.19GB\n",
      "----------------------------------------\n",
      "基础方法: 0.77GB\n",
      "ZeRO-1: 0.77GB (0.0% 节省)\n",
      "ZeRO-2: 0.34GB (55.0% 节省)\n",
      "ZeRO-3: 0.20GB (73.4% 节省)\n",
      "CPU Offload: 0.39GB (48.9% 节省)\n"
     ]
    }
   ],
   "source": [
    "# 汇总所有方法的显存使用情况\n",
    "def compare_methods():\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "\n",
    "    print(\"\\n 显存使用对比 (单位: GB):\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 重新运行基础测试\n",
    "    baseline = analyze_memory()\n",
    "    print(\"-\" * 40)\n",
    "    zero1 = test_zero1()\n",
    "    print(\"-\" * 40)\n",
    "    zero2 = test_zero2()\n",
    "    print(\"-\" * 40)\n",
    "    zero3 = test_zero3()\n",
    "    print(\"-\" * 40)\n",
    "    offload = test_cpu_offload()\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 提取最终显存使用量\n",
    "    print(f\"基础方法: {baseline['allocated'][-1]:.2f}GB\")\n",
    "    print(f\"ZeRO-1: {zero1['allocated'][-1]:.2f}GB ({(1-zero1['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% 节省)\")\n",
    "    print(f\"ZeRO-2: {zero2['allocated'][-1]:.2f}GB ({(1-zero2['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% 节省)\")\n",
    "    print(f\"ZeRO-3: {zero3['allocated'][-1]:.2f}GB ({(1-zero3['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% 节省)\")\n",
    "    print(f\"CPU Offload: {offload['allocated'][-1]:.2f}GB ({(1-offload['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% 节省)\")\n",
    "\n",
    "# 执行对比\n",
    "compare_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a33059",
   "metadata": {},
   "source": [
    "通过这个实验，我们可以清楚地看到 ZeRO 各级别对显存占用的优化效果。在实际的大模型训练中，这些优化可以带来数倍甚至数十倍的显存节省。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbd6f1",
   "metadata": {},
   "source": [
    "```\n",
    "显存使用对比 (单位: GB):\n",
    "----------------------------------------\n",
    "基础方法: 0.39GB\n",
    "ZeRO-1: 0.39GB (0.0% 节省)\n",
    "ZeRO-2: 0.31GB (20.5% 节省)\n",
    "ZeRO-3: 0.11GB (71.8% 节省)\n",
    "CPU Offload: 0.25GB (35.9% 节省)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a22f1",
   "metadata": {},
   "source": [
    "## 总结与思考\n",
    "\n",
    "ZeRO 技术通过分片优化器状态、梯度和参数，显著降低了大模型训练的显存需求。本实验通过代码实现和原理分析，深入探讨了：\n",
    "\n",
    "1. **ZeRO-1**：优化器状态分片，减少约 4 倍显存占用\n",
    "2. **ZeRO-2**：梯度分片，进一步减少约 8 倍显存占用  \n",
    "3. **ZeRO-3**：参数分片，最大可减少约 N 倍显存占用（N 为 GPU 数量）\n",
    "4. **Zero Offload**：将数据卸载到 CPU/NVMe，支持训练超大模型\n",
    "\n",
    "这些技术可以组合使用，根据具体的硬件环境和模型大小选择最合适的配置。在实际应用中，DeepSpeed 框架提供了完整的 ZeRO 实现，建议直接使用经过优化的官方实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a71e6",
   "metadata": {},
   "source": [
    "## 引用与参考\n",
    "\n",
    "- https://arxiv.org/abs/1910.02054\n",
    "- https://www.cnblogs.com/whiteBear/p/18341975"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
