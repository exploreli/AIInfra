{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e6ce38",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 04: 专家并行大规模训练\n",
    "\n",
    "> Author by: 许灿岷\n",
    "\n",
    "本实验旨在深入理解混合专家模型(MoE)架构中的专家并行技术，掌握大规模模型训练的基本原理，并实践权重转换技术。通过简化但完整的代码实现，帮助初学者理解 MoE 架构的核心概念和实现方法。\n",
    "\n",
    "## 1. MoE 模型原理\n",
    "\n",
    "MoE 模型是一种将多个\"专家\"网络组合成一个大型模型的技术。每个专家是一个相对较小的神经网络，专门处理特定类型或特定分布的数据。MoE 的核心思想是通过一个门控网络(gating network)来决定每个输入应该被路由到哪些专家进行处理。\n",
    "\n",
    "在 MoE 中，前向传播可以表示为：\n",
    "\n",
    "$$y = \\sum_{i=1}^{n} G(x)_i \\cdot E_i(x)$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $E_i$ 是第 i 个专家网络\n",
    "- $G(x)_i$ 是门控网络对于第 i 个专家的输出权重\n",
    "- $n$ 是专家数量\n",
    "\n",
    "门控网络通常使用 softmax 函数确保所有权重之和为 1：\n",
    "\n",
    "$$G(x) = \\text{softmax}(W_g \\cdot x + b_g)$$\n",
    "\n",
    "![](./images/Code04Expert01.png)\n",
    "\n",
    "## 2. 基础 MoE 层实现\n",
    "\n",
    "首先，我们实现一个基础的 MoE 层，包含多个专家和一个门控网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f02b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([8, 10])\n",
      "输出形状: torch.Size([8, 5])\n",
      "MoE 层参数数量: 4160\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicMoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    基础 MoE 层实现\n",
    "    包含多个专家网络和一个门控网络\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, num_experts, expert_hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # 创建专家网络 - 每个专家是一个简单的 MLP\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, expert_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(expert_hidden_size, output_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # 门控网络 - 决定输入如何分配给专家\n",
    "        self.gate = nn.Linear(input_size, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        1. 门控网络计算每个专家的权重\n",
    "        2. 选择 top-k 专家\n",
    "        3. 计算专家输出并加权组合\n",
    "        \"\"\"\n",
    "        # 计算门控权重\n",
    "        gate_scores = self.gate(x)\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "\n",
    "        # 选择 top-2 专家\n",
    "        top2_probs, top2_indices = torch.topk(gate_probs, k=2, dim=-1)\n",
    "\n",
    "        # 归一化选中的专家权重\n",
    "        top2_probs = top2_probs / top2_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # 计算最终输出（修正：初始化时指定设备，与输入保持一致）\n",
    "        final_output = torch.zeros(x.size(0), self.output_size, device=x.device)\n",
    "\n",
    "        # 对每个选中的专家计算输出并加权组合\n",
    "        for i in range(2):\n",
    "            expert_idx = top2_indices[:, i]\n",
    "            expert_mask = torch.zeros_like(gate_probs)\n",
    "\n",
    "            # 创建专家掩码\n",
    "            for j in range(x.size(0)):\n",
    "                expert_mask[j, expert_idx[j]] = 1\n",
    "\n",
    "            # 计算当前专家组的输出\n",
    "            expert_outputs = []\n",
    "            for idx in range(self.num_experts):\n",
    "                expert_input = x[expert_mask[:, idx].bool()]\n",
    "                if expert_input.size(0) > 0:\n",
    "                    # 修正原代码中的语法错误：self.expertsexpert_input → self.experts[idx](expert_input)\n",
    "                    expert_out = self.experts[idx](expert_input)\n",
    "                    expert_outputs.append((expert_out, expert_mask[:, idx].bool()))\n",
    "\n",
    "            # 组合专家输出\n",
    "            for expert_out, mask in expert_outputs:\n",
    "                final_output[mask] += expert_out * top2_probs[mask, i].unsqueeze(1)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "# 测试基础 MoE 层\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "num_experts = 4\n",
    "batch_size = 8\n",
    "\n",
    "moe_layer = BasicMoELayer(input_size, output_size, num_experts)\n",
    "x = torch.randn(batch_size, input_size)\n",
    "output = moe_layer(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"MoE 层参数数量: {sum(p.numel() for p in moe_layer.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d899a",
   "metadata": {},
   "source": [
    "这个基础 MoE 层实现展示了 MoE 模型的核心概念。每个专家是一个独立的小型神经网络，门控网络决定输入如何分配给不同的专家。在实际应用中，MoE 可以显著增加模型容量而不显著增加计算成本，因为每个输入只使用少数专家。\n",
    "\n",
    "## 3. 专家并行实现\n",
    "\n",
    "专家并行是将不同专家分布到不同设备上的技术，允许模型规模超过单个设备的内存限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bee9918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "专家并行 MoE 输出形状: torch.Size([8, 5])\n",
      "专家设备分配: ['cuda:0', 'cuda:1', 'cuda:0', 'cuda:1']\n"
     ]
    }
   ],
   "source": [
    "class ExpertParallelMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    专家并行 MoE 实现\n",
    "    将不同专家分布到不同设备上，采用批量处理提高效率\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, num_experts, expert_hidden_size=64, device_ids=None):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # 设置设备分配（增强：自动适配 CPU/GPU 环境）\n",
    "        if device_ids is None:\n",
    "            if torch.cuda.is_available():\n",
    "                device_ids = list(range(min(num_experts, torch.cuda.device_count())))\n",
    "            else:\n",
    "                device_ids = [torch.device('cpu')]\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        # 创建专家网络并分配到不同设备\n",
    "        self.experts = nn.ModuleList()\n",
    "        self.expert_devices = []  # 记录每个专家所在的设备\n",
    "\n",
    "        for expert_idx in range(num_experts):\n",
    "            device_id = device_ids[expert_idx % len(device_ids)]\n",
    "            # 确保设备 ID 是有效的设备对象或索引\n",
    "            if isinstance(device_id, str):\n",
    "                device = torch.device(device_id)\n",
    "            else:\n",
    "                device = torch.device(\n",
    "                    f\"cuda:{device_id}\" if device_id != -1 and torch.cuda.is_available() else \"cpu\"\n",
    "                )\n",
    "\n",
    "            expert = nn.Sequential(\n",
    "                nn.Linear(input_size, expert_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(expert_hidden_size, output_size)\n",
    "            ).to(device)\n",
    "\n",
    "            self.experts.append(expert)\n",
    "            self.expert_devices.append(device)\n",
    "\n",
    "        # 门控网络（在主设备上）\n",
    "        self.main_device = self.expert_devices[0]\n",
    "        self.gate = nn.Linear(input_size, num_experts).to(self.main_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        专家并行的前向传播（优化版）\n",
    "        批量处理而非单个样本处理，提高效率\n",
    "        \"\"\"\n",
    "        # 确保输入在主设备上\n",
    "        x = x.to(self.main_device)\n",
    "\n",
    "        # 计算门控权重\n",
    "        gate_scores = self.gate(x)\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "\n",
    "        # 选择 top-2 专家\n",
    "        top2_probs, top2_indices = torch.topk(gate_probs, k=2, dim=-1)\n",
    "        top2_probs = top2_probs / top2_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # 初始化输出张量\n",
    "        final_output = torch.zeros(x.size(0), self.output_size, device=self.main_device)\n",
    "\n",
    "        # 按专家 ID 分组处理，而不是逐个样本处理（核心优化）\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # 找出所有选择了当前专家的样本和位置\n",
    "            for k in range(2):  # 对 top-1 和 top-2 专家分别处理\n",
    "                mask = (top2_indices[:, k] == expert_idx)\n",
    "                if mask.sum() == 0:\n",
    "                    continue  # 没有样本选择该专家\n",
    "\n",
    "                # 获取对应样本和权重\n",
    "                selected_x = x[mask]\n",
    "                selected_probs = top2_probs[mask, k].unsqueeze(1)\n",
    "\n",
    "                # 将样本发送到专家所在设备\n",
    "                expert_device = self.expert_devices[expert_idx]\n",
    "                x_on_device = selected_x.to(expert_device)\n",
    "\n",
    "                # 计算专家输出\n",
    "                expert_output = self.experts[expert_idx](x_on_device)\n",
    "\n",
    "                # 将结果发送回主设备并加权\n",
    "                final_output[mask] += expert_output.to(self.main_device) * selected_probs\n",
    "\n",
    "        return final_output\n",
    "\n",
    "# 测试专家并行 MoE\n",
    "device_ids = [0, 1] if torch.cuda.device_count() >= 2 else ['cpu']\n",
    "parallel_moe = ExpertParallelMoE(input_size, output_size, num_experts, device_ids=device_ids)\n",
    "\n",
    "x = torch.randn(batch_size, input_size)\n",
    "output = parallel_moe(x)\n",
    "\n",
    "print(f\"专家并行 MoE 输出形状: {output.shape}\")\n",
    "print(f\"专家设备分配: {[str(dev) for dev in parallel_moe.expert_devices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbaf60d",
   "metadata": {},
   "source": [
    "专家并行实现展示了如何将不同专家分布到不同设备上。这种并行策略的关键优势是允许模型规模超过单个设备的内存限制，同时保持相对较高的计算效率，因为每个输入只使用少数专家。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc902872",
   "metadata": {},
   "source": [
    "## 4. 权重转换与模型缩放\n",
    "\n",
    "在大规模模型训练中，权重转换技术用于在不同并行策略间转换模型参数，或者将小模型权重扩展到更大模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1acb79cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小模型参数数量: 2080\n",
      "大模型参数数量: 4160\n",
      "扩展后模型参数数量: 4160\n",
      "扩展模型输出形状: torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "def expand_model_weights(small_model, large_model, expansion_factor=2):\n",
    "    \"\"\"\n",
    "    将小模型权重扩展到大模型\n",
    "    适用于 MoE 模型的专家扩展\n",
    "    \"\"\"\n",
    "    small_state_dict = small_model.state_dict()\n",
    "    large_state_dict = large_model.state_dict()\n",
    "\n",
    "    # 复制共享参数\n",
    "    for name, param in small_state_dict.items():\n",
    "        if name in large_state_dict and param.size() == large_state_dict[name].size():\n",
    "            large_state_dict[name] = param\n",
    "\n",
    "    # 扩展专家权重\n",
    "    for name, param in small_state_dict.items():\n",
    "        if 'experts' in name:\n",
    "            # 获取专家索引\n",
    "            expert_idx = int(name.split('.')[1])  # 假设名称格式为 \"experts.0.weight\"\n",
    "\n",
    "            # 复制到多个专家\n",
    "            for i in range(expansion_factor):\n",
    "                new_expert_idx = expert_idx * expansion_factor + i\n",
    "                new_name = name.replace(f\"{expert_idx}\", f\"{new_expert_idx}\")\n",
    "\n",
    "                if new_name in large_state_dict:\n",
    "                    large_state_dict[new_name] = param.clone()\n",
    "\n",
    "    # 加载扩展后的权重\n",
    "    large_model.load_state_dict(large_state_dict)\n",
    "    return large_model\n",
    "\n",
    "# 创建小模型和大模型\n",
    "small_moe = BasicMoELayer(input_size, output_size, num_experts=2)\n",
    "large_moe = BasicMoELayer(input_size, output_size, num_experts=4)\n",
    "\n",
    "print(f\"小模型参数数量: {sum(p.numel() for p in small_moe.parameters())}\")\n",
    "print(f\"大模型参数数量: {sum(p.numel() for p in large_moe.parameters())}\")\n",
    "\n",
    "# 扩展权重\n",
    "expanded_moe = expand_model_weights(small_moe, large_moe, expansion_factor=2)\n",
    "print(f\"扩展后模型参数数量: {sum(p.numel() for p in expanded_moe.parameters())}\")\n",
    "\n",
    "# 测试扩展后的模型\n",
    "x = torch.randn(batch_size, input_size)\n",
    "output = expanded_moe(x)\n",
    "print(f\"扩展模型输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e3b64",
   "metadata": {},
   "source": [
    "权重转换技术在大规模模型训练中非常重要，它允许我们从小模型开始训练，然后扩展到更大的模型，或者在不同并行配置间转换模型参数。这种方法可以显著减少训练时间和计算资源需求。\n",
    "\n",
    "## 5. 简化的大规模训练实践\n",
    "\n",
    "下面是一个简化的大规模训练示例，展示了如何使用 MoE 和专家并行进行模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c635eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 73.8583, Accuracy: 10.20%\n",
      "Epoch 2/5, Loss: 71.8026, Accuracy: 19.40%\n",
      "Epoch 3/5, Loss: 70.1894, Accuracy: 23.30%\n",
      "Epoch 4/5, Loss: 68.5149, Accuracy: 25.80%\n",
      "Epoch 5/5, Loss: 66.2796, Accuracy: 29.10%\n"
     ]
    }
   ],
   "source": [
    "def train_moe_model():\n",
    "    \"\"\"\n",
    "    简化的大规模 MoE 模型训练示例\n",
    "    \"\"\"\n",
    "    # 模型参数\n",
    "    input_size = 20\n",
    "    hidden_size = 64\n",
    "    output_size = 10\n",
    "    num_experts = 8\n",
    "    num_classes = 10\n",
    "\n",
    "    # 创建 MoE 模型\n",
    "    moe_model = BasicMoELayer(input_size, hidden_size, num_experts)\n",
    "    classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    # 创建优化器\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(moe_model.parameters()) + list(classifier.parameters()),\n",
    "        lr=0.001\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 模拟训练数据\n",
    "    num_samples = 1000\n",
    "    x_data = torch.randn(num_samples, input_size)\n",
    "    y_data = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "    # 训练循环\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # 获取批次数据\n",
    "            x_batch = x_data[i:i+batch_size]\n",
    "            y_batch = y_data[i:i+batch_size]\n",
    "\n",
    "            # 前向传播\n",
    "            moe_output = moe_model(x_batch)\n",
    "            class_output = classifier(moe_output)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(class_output, y_batch)\n",
    "\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(class_output.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return moe_model, classifier\n",
    "\n",
    "# 运行训练\n",
    "moe_model, classifier = train_moe_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783ebbb",
   "metadata": {},
   "source": [
    "这个简化的大规模训练示例展示了如何使用 MoE 模型进行训练。在实际的大规模训练中，还需要考虑数据并行、梯度累积、学习率调度等更复杂的技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7155b",
   "metadata": {},
   "source": [
    "## 6. 专家负载均衡\n",
    "\n",
    "在 MoE 模型中，专家负载均衡是一个重要问题，需要确保所有专家都能得到充分利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a66aac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "专家使用情况:\n",
      "专家 0: 26.0 次 使用，使用率: 13.00%\n",
      "专家 1: 36.0 次 使用，使用率: 18.00%\n",
      "专家 2: 19.0 次 使用，使用率: 9.50%\n",
      "专家 3: 30.0 次 使用，使用率: 15.00%\n",
      "专家 4: 21.0 次 使用，使用率: 10.50%\n",
      "专家 5: 13.0 次 使用，使用率: 6.50%\n",
      "专家 6: 31.0 次 使用，使用率: 15.50%\n",
      "专家 7: 24.0 次 使用，使用率: 12.00%\n",
      "负载均衡熵值: 2.0398\n"
     ]
    }
   ],
   "source": [
    "def calculate_expert_usage(moe_layer, x, top_k=2):\n",
    "    with torch.no_grad():\n",
    "        gate_scores = moe_layer.gate(x)\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "\n",
    "        # 选择 top-k 专家\n",
    "        topk_probs, topk_indices = torch.topk(gate_probs, k=top_k, dim=-1)\n",
    "\n",
    "        # 计算每个专家的使用次数\n",
    "        expert_usage = torch.zeros(moe_layer.num_experts)\n",
    "        for i in range(x.size(0)):\n",
    "            for j in range(top_k):\n",
    "                expert_idx = topk_indices[i, j].item()\n",
    "                expert_usage[expert_idx] += 1\n",
    "\n",
    "        # 计算使用率（使用次数/总可能使用次数）\n",
    "        total_possible = x.size(0) * top_k\n",
    "        expert_utilization = expert_usage / total_possible\n",
    "\n",
    "        # 计算负载均衡指标（熵值，值越高表示越均衡）\n",
    "        if total_possible > 0:\n",
    "            usage_distribution = expert_usage / total_possible\n",
    "            entropy = -torch.sum(usage_distribution * torch.log(usage_distribution + 1e-10))\n",
    "        else:\n",
    "            entropy = 0.0\n",
    "\n",
    "        return expert_usage, expert_utilization, entropy\n",
    "\n",
    "# 1. 统一用训练时的 input_size\n",
    "input_size = 20          # 与 train_moe_model() 里保持一致\n",
    "x_test = torch.randn(100, input_size)\n",
    "\n",
    "# 3. 计算负载\n",
    "expert_usage, expert_utilization, entropy = calculate_expert_usage(moe_model, x_test)\n",
    "\n",
    "print(\"专家使用情况:\")\n",
    "for i in range(len(expert_usage)):\n",
    "    print(f\"专家 {i}: {expert_usage[i].item()}次 使用，使用率: {expert_utilization[i].item():.2%}\")\n",
    "print(f\"负载均衡熵值: {entropy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3513c",
   "metadata": {},
   "source": [
    "专家负载均衡是 MoE 模型中的一个关键问题。如果某些专家很少被使用，而其他专家过度使用，会导致模型效率低下。在实际应用中，通常需要添加辅助损失函数来鼓励更均衡的专家使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39dea40",
   "metadata": {},
   "source": [
    "## 总结与思考\n",
    "\n",
    "MoE 技术的核心价值在于能够创建参数量极大但计算效率较高的模型。通过将大型模型分解为多个专家，每个输入只使用少数专家，MoE 可以在保持合理计算成本的同时显著增加模型容量。\n",
    "\n",
    "专家并行是 MoE 的自然扩展，允许将不同专家分布到不同设备上，从而支持超大规模模型的训练。权重转换技术则提供了模型扩展和并行策略转换的灵活性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
