# 大模型蒸馏技术

Author by：曾薇敏

## 为什么大模型需要蒸馏

近年来，大语言模型（LLMs）迅速发展，以 GPT-5 与 Gemini 为代表的**专有模型**在参数规模和能力上持续突破，展现出涌现能力，能够处理远超其原始训练目标的任务，这些模型在上下文理解、复杂推理以及生成任务中表现卓越。然而，其高度闭源性与商业化属性带来了诸多限制：首先，**可访问性有限和成本较高**，使得个人和中小型组织难以承受；其次，使用过程中需将敏感数据传输至外部服务器，引发**数据隐私与安全风险**；最后，其通用性虽强，但缺乏**对特定领域任务的适配性**。因此，可访问性、成本和适应性方面的限制在充分发挥专有 LLMs 的潜力方面构成了重大挑战。

**开源大模型**因其**开放性与可定制性**而在人工智能研究中扮演着重要角色，使个人研究者和中小型组织能够以较低成本获取并应用先进模型。早期由于训练资源和数据规模的限制，开源模型在指令理解和复杂任务上的表现一度远落后于专有大模型。然而，随着大模型蒸馏技术的发展，这一差距正在快速缩小。通过知识蒸馏等技术手段，开源模型能够高效吸收专有模型的知识表示与推理模式，在很多下游任务上已能与专有模型接近。

大模型蒸馏不仅推动了开源模型在性能上的飞跃，也改善了其在计算效率、能耗和可部署性方面的表现。更重要的是，大模型蒸馏促进了人工智能的普惠化，使先进能力能够在更广泛的群体中传播与应用，推动了多样化和公平化的技术生态。

综上所述，对大型语言模型蒸馏的需求日益增长，源于人工智能领域（如 OpenAI 等）的快速演变以及这些模型复杂性的不断增加。随着人工智能不断渗透各个领域，将专有大型语言模型的知识高效有效地蒸馏到开源模型中的需求促使大模型蒸馏技术应运而生。这一需求是由对更易于获取、更具成本效益和适应性更强的 AI 解决方案日益增长的需求所驱动的，并且这些解决方案要能够满足各种特定垂直领域的应用需求。随着人工智能技术持续扩展至各个行业，大模型蒸馏已成为平衡性能、成本与安全性的必要手段，也是推动人工智能生态可持续发展的重要驱动力。

## 什么是大模型蒸馏

知识蒸馏是人工智能与深度学习领域的一项核心技术，其本质在于将大型复杂模型中的知识与能力迁移到更小、更高效的模型之中。这一过程源于早期的神经网络研究，最初的目标是通过模仿教师模型的输出，来训练出能够在资源受限环境中部署的学生模型。

随着大语言模型的兴起，知识蒸馏（KD）在大型语言模型（LLMs）中发挥着三个关键作用：

1. 主要为了**增强能力（advance）**，即通过从教师模型中提取和迁移知识，使学生模型能够掌握更高层次的推理与认知模式；

2. 提供传统的**压缩作用（compress）**，在保持性能的同时减少模型规模和计算开销，提高运行效率；

3. 一种新兴趋势是通过**自我生成知识实现自我改进（self-improvement）**，即利用模型自身生成的数据不断迭代训练，从而实现能力的持续提升。

![iigngsljglsngjglsn](./images/3_role.png)

LLMs 的通用知识蒸馏流程是⼀个结构化、系统化的过程，旨在将知识从复杂的教师模型迁移到较简单的学⽣模型。如下图所示，整体流程可分为四个阶段，每一阶段均对应着蒸馏在不同功能维度上的作用。

![图 1：蒸馏过程的 3 个作用](./images/蒸馏流程.png)

第一阶段是**目标技能或领域的引导**。教师模型通过提示或模板被聚焦到特定的知识范围或能力类型。与早期的无差别模仿不同，这一过程强调对教师知识的定向挖掘。例如，将教师模型限定在医学、法律等专业领域，或推理、语言理解等能力上。

第二阶段是**种子知识的注入**。种子知识通常是小规模的输入数据集，包含与目标领域相关的少量知识实例或线索。其作用是为教师模型提供生成的起点，使其能够基于有限的输入扩展出更大规模、更系统的训练样本。

第三阶段是**蒸馏知识的生成**。教师模型在指令和种子知识的引导下，生成符合目标领域的示例。示例不仅包括答案，还可能包含推理链条与解释性内容。教师模型将自身的隐性推理能力外化为显性数据，使学生模型能够从中学习如何进行复杂思考。

第四阶段是**学生模型的训练与优化**。生成的蒸馏数据被用作训练语料，学生模型在损失函数的约束下不断调整参数，以最小化与教师输出之间的差异。

总体而言，大模型蒸馏流程是一个多维度的知识迁移框架。它通过目标引导和种子扩展实现能力拓展，通过训练优化实现性能压缩。随着开源大模型的发展，蒸馏已不再局限于单向的“教师—学生”模式。利用开源模型自身作为教师角色，通过生成数据和自蒸馏的方式不断迭代更新已经成为新的趋势。

## 主流蒸馏方法分类

当前，针对大规模语言模型的知识蒸馏方法呈现多样化趋势，下面是一些比较典型的大模型蒸馏的类别。

**稀疏→稠密蒸馏**：稀疏→稠密蒸馏指使用稀疏的混合专家（Mixture-of-Experts, MoE）教师模型，将其知识迁移到结构致密的学生模型的过程。

典型流程通常分为**知识聚合**和**标准蒸馏**两阶段，先将不同专家的知识（如参数或输出）进行汇总，即聚合预训练专家知识，然后再通过常规的蒸馏损失对学生模型进行微调。知识聚合方式包括线性叠加和层次融合等。常用知识聚合策略包括对专家参数加权求和、取平均、Top-K 聚合或奇异值分解等方式收集知识，然后以 KL 散度等蒸馏损失细化学生。目标是在保持高性能的同时，获得一个硬件友好的稠密模型。此外，针对 MoE 特性，还有方法（“Every Expert Matters”）提出了专用的蒸馏机制，通过知识增强（KA）学生感知路由器（SAR）未激活专家的知识，让学生模型能更有效地利用所有专家的输出，这在 ROUGE-L 等指令遵循评估中被证明能显著优于常规 KD 基线方法。

**多模态蒸馏**：多模态蒸馏指从包含多种模态信息（如视觉+语言）的教师模型向学生模型传递跨模态知识的过程。其基本机制通常包括对不同模态的表示进行对齐和融合，以使学生学习教师在视觉、语言等任务上的综合能力。

常见方法需要设计多模态对齐损失，包括跨模态表示对齐蒸馏和模态间关系蒸馏等，如**表示对齐蒸馏**通过最小化视觉和语言特征的距离（例如 LLaVA-KD 中的 MDist 将视觉表征与语言表征对齐）；**关系蒸馏**则关注元素之间的关系保持（如 LLaVA-KD 中的 RDist 对视觉令牌关系进行学习）。训练策略包括联合蒸馏和分阶段蒸馏，LLaVA-KD 采用三阶段训练（蒸馏预训练、监督微调、蒸馏微调），并结合多模态输出蒸馏（MDist）和视觉关系蒸馏（RDist），以同时对齐学生模型的视觉特征和输出分布。还存在基于隐式视觉知识的方案，如 VIKDF (Visual Implicit Knowledge Distillation Framework) 通过 IQ-Former 提取视觉隐式知识，并利用 BVIF 技术将知识融入 LLM，使其能够生成连贯、富有情境理解的对上表展示了 Distinct-4 指标和语言模型损失（多样性分析）。

**数据蒸馏**：“数据蒸馏”是数据增强 (Data Augmentation, DA) 与 KD 框架结合的强大范式，包括数据集蒸馏，伪标签数据蒸馏，合成数据蒸馏，任务驱动数据蒸馏等。**数据集蒸馏**旨在将大规模训练集压缩为极小的合成样本集合，同时保留原数据的训练效果；蒸馏流程通常先使用教师模型在原始大数据上学习，然后将该知识通过一小批高质量数据（原始子集或合成样本）传递给学生。

数据蒸馏方法主要可分为**基于优化的数据压缩**与**合成样本**生成两大类。前者采用梯度匹配、轨迹匹配等元学习技术迭代优化小数据集，使学生在该数据上训练时近似原数据训练效果；后者则利用生成模型或教师模型反演等手段直接合成语料（如使用 MMD 匹配分布或 Prompt 引导大模型生成数据）。另一种趋势是提示驱动的合成数据，通过设计提示词让大模型生成任务相关的伪标注数据，以此训练学生。如 UltraChat 通过收集跨领域（如关于世界的提问、创作与生成、现有材料协助）的元信息，指导教师 LLM 蒸馏出具有词汇和主题多样性的大规模指令和对话数据。还有 Phi 系列 (Phi-1, Phi-1.5)：专注于蒸馏“教科书质量”的数据集，例如在编码领域，蒸馏出清晰、独立、具有指导意义且平衡的内容，以小数据集实现卓越性能。

**与 RL 结合**：结合强化学习的蒸馏在学生模型训练中同时引入教师监督和环境奖励，将知识蒸馏和强化学习目标统一优化。实验证明相比单独使用 RL 或 KD，联合训练能更高效地提升模型推理能力。例如，LLMR 方法通过从教师 LLM 的预测概率导出 q 值并构造奖励函数，然后用策略梯度方法让学生根据该奖励进行训练，使用反向散度缓解了暴露偏差问题。类似地，DeepSeek 团队使用基于 RL 优化的教师模型生成高质量推理示例，然后通过传统的监督蒸馏让小模型学习这些知识。
该类方法需要设计**联合损失**或**奖励塑形**，将蒸馏损失（如 KL 散度）与环境回报结合。奖励塑形是指在原始奖励中加入教师监督得到的 KL 项，使学生的策略优化既考虑任务回报也模仿教师；联合损失优化是指将传统交叉熵或 KL 蒸馏损失作为 RL 的辅助损失同时最小化。**直接偏好优化**（Direct Preference Optimization）DPO 是一种简化 RL 目标的方法，它将涉及奖励最大化和 KL 散度约束的强化学习目标流线化为单阶段的策略训练。例如，Zephyr 模型 就利用 DPO 蒸馏教师 LLM 中的偏好对齐知识。

## 主流大模型蒸馏算法与效果

**MiniLLM**：MiniLLM 框架的核心方法的整体思路是将蒸馏目标从传统的正向 KL 散度替换为反向 KL 散度（RKL），这一机制被应用于白盒知识蒸馏，避免学生模型高估教师分布的长尾概率。其通过策略梯度推导，并设计了三项关键策略——单步分解（减少方差，通过精确期望计算单步质量）、教师混合采样（抑制奖励作弊，提高采样质量）以及长度归一化（解决长度偏差，避免偏好短文本）。在不同模型族与多种规模上，系统性地评估了 MiniLLM 框架的性能。结果显示，MiniLLM 在生成质量、长文本一致性、概率校准性（ECE 低）等方面显著优于传统 KD 和 SFT，并且通过在策略采样有效缓解了暴露偏差。MiniLLM 的性能与教师模型规模呈正相关，部分学生模型甚至超过教师模型表现。

传统 KD 方法普遍基于**正向 KL 散度 $KL[p‖q]$** 作为优化目标，即令学生模型尽可能覆盖教师模型的概率分布。这种“模式覆盖（mode-covering）”特性在分类任务中通常有效，但在开放式文本生成任务中却带来了严重问题：教师模型的分布往往包含大量长尾模式，而学生模型容量有限，被迫去拟合低概率区域，容易导致生成质量下降。更具体而言，学生模型在训练时过度追逐教师的长尾分布，推理时则表现为**暴露偏差（exposure bias）**、文本不连贯以及生成内容的置信度偏差。以**反向 KL 散度 $KL[q‖p]$ 替代正向 KL 散度**作为蒸馏目标。与正向 KL 的模式覆盖不同，反向 KL 具有**模式寻求（mode-seeking）** 的特性，使学生模型更专注于教师分布的高概率区域，而非追逐长尾噪声。

**算法简介**：

MiniLLM 的蒸馏流程可概括为以下几个关键步骤：首先在输入与初始化阶段，准备指令数据集 D（用于蒸馏对齐）、预训练语料 $D_{PT}$​（保持语言建模能力）、教师模型 p、以及已在 $D_{PT}$​ 上预训练的学生模型 $q_{θ0​​}$，并设定学习率、批大小和梯度裁剪阈值等超参数。通过这一流程，学生模型不仅继承了教师分布的高概率模式，还在长文本生成、一致性与校准性上超越了传统蒸馏方法为了让学生模型具备基本的指令遵循能力，算法先在 D 上进行一次监督微调，选取验证集损失最低的参数 θ 作为起点。进入核心的迭代蒸馏阶段，每轮训练会从 D 中采样提示并获取教师响应，同时从 $D_{PT}$ 中采样长文档文本。接着计算三类梯度：一是基于反向 KL 的单步分解梯度；二是长度归一化梯度；三是语言建模梯度。最后，将这三类梯度（以及 PPO 风格的裁剪）结合起来更新参数 θ，不断迭代直至收敛。

![算法图片](./images/Algorithm2.png)

**实验结果**：

![](./images/table1-2.png)

上图所示为评估结果。GPT4 和 R-L 分别表示在 5 个随机种子下的 GPT-4 平均反馈分数和 Rouge-L 分数。每种模型规模的最佳分数用加粗字体表示，当学生模型表现超过教师模型，其分数则标记为 *。

实验结果表明，MiniLLM 在多个规模和模型族下均显著优于基线方法。与传统 KD 和 SeqKD 相比，MiniLLM 更好地对齐教师分布，在 Rouge-L 和 GPT-4 偏好评测中均取得领先。在部分规模下（如 GPT-2 760M 的学生模型），MiniLLM 的表现甚至超过了对应的教师模型。MiniLLM 生成的文本在流畅性、相关性和逻辑性上普遍优于 SFT、KD 与 SeqKD。

![](./images/table1-4.png)

如上图，基于 LLaMA 系列模型，在 SelfInst 数据集上的结果中，MiniLLM 在人类偏好方面优于所有基线方法，并与教师模型表现相当。

![](./images/Figure6.png)

上图是训练-解码差异导致的累积超额错误（ExAccErr）随生成长度的变化，MiniLLM 的 ExAccErr 显著更低，并在长文本生成（>150 tokens）中误差停止累积，表明反向 KL 的模式寻求特性有效缓解了暴露偏差。

![](./images/Figure7.png)

上图是在 S-NI 数据集上按响应长度划分的子集上，蒸馏模型相对于 SFT 的 Rouge-L 评分。在需要较长响应（≥6 tokens）的任务中，MiniLLM 相比标准 KD 模型显示出明显优势，但在短响应任务上性能相似。

![](./images/table1-22.png)

上表是 SST2 和 BoolQ 数据集上的 ECE（Expected Calibration Error） 和准确率得分（校准性分析）。MiniLLM 的校准性显著优于基线方法（ECE 更低），并且准确率更高。

![](./images/table1-3.png)

上表展示了 Distinct-4 指标和语言模型损失（多样性分析）。在 Distinct-4 指标上，MiniLLM 与基线方法相近，说明反向 KL 的模式寻求特性并未导致明显的模式坍缩。学生模型既能保证生成质量，也能维持基本的多样性。

实验结果清晰地展示了 MiniLLM 框架的有效性与稳健性。通过引入反向 KL 与三项稳定训练策略，MiniLLM 不仅在整体生成质量上超过传统蒸馏方法，还显著改善了长文本一致性与概率校准。同时，学生模型的多样性未受到明显削弱，表明模式寻求与生成多样性之间可以取得良好平衡。

**GKD**:

**LM-Cocktail**（混合专家）：

**SDD(逐层动态蒸馏)**:

## 蒸馏效果评估

## 实例：DeepSeek 蒸馏技术解读

## 总结与思考

## 参考文献

[1] Xu X, Li M, Tao C, et al. A survey on knowledge distillation of large language models[J]. arXiv preprint arXiv:2402.13116, 2024.

[2] Gu Y, Dong L, Wei F, et al. Minillm: Knowledge distillation of large language models[J]. arXiv preprint arXiv:2306.08543, 2023.

[3] Liu X, Hu L, Bailis P, et al. Online speculative decoding[J]. arXiv preprint arXiv:2310.07177, 2023.

[4] Kim G, Chu G, Yang E. Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models[J]. arXiv preprint arXiv:2502.12947, 2025.

[5] Xue F, He X, Ren X, et al. One student knows all experts know: From sparse to dense[J]. arXiv preprint arXiv:2201.10890, 2022.

[6] Cai Y, Zhang J, He H, et al. Llava-kd: A framework of distilling multimodal large language models[J]. arXiv preprint arXiv:2410.16236, 2024.

[7] Zhang B, Ma H, Ding J, et al. Distilling implicit multimodal knowledge into large language models for zero-resource dialogue generation[J]. Information Fusion, 2025, 118: 102985.

[8] Fang L, Yu X, Cai J, et al. Knowledge distillation and dataset distillation of large language models: Emerging trends, challenges, and future directions[J]. arXiv preprint arXiv:2504.14772, 2025.

[9] Xu H, Zhu Q, Deng H, et al. KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning[J]. arXiv preprint arXiv:2506.02208, 2025.

[10] Guo D, Yang D, Zhang H, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning[J]. arXiv preprint arXiv:2501.12948, 2025.
