{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FastGen 入门实战 Notebook（代码内含详细中文注释）\n",
        "作者：汪袁烁 \n",
        "\n",
        "目标：用带注释的可运行示例 + 框架 demo，让你一边读注释一边跑代码，就能理解 FastGen / vLLM / DeepSpeed-MII 机制。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境准备与库导入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装所需库（在 notebook / 虚拟环境中执行）\n",
        "# 如果你已经在环境中安装了部分，可以跳过或注释掉相应行\n",
        "\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# PyTorch：根据你的 CUDA 版本选择对应的包，这里使用通用版本（可能是 CPU 或 GPU 版本）\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "# 用于 notebook 中画图\n",
        "!pip install matplotlib\n",
        "\n",
        "# DeepSpeed-MII，是 FastGen 的一部分，用于高性能推理  \n",
        "!pip install deepspeed-mii\n",
        "\n",
        "# vLLM，用于启用 chunked prefill 的示例  \n",
        "!pip install vllm\n",
        "\n",
        "# 如果你还打算尝试 transformers / huggingface 接口的话，也可以加上：\n",
        "!pip install transformers\n",
        "\n",
        "# （可选）如果用 tokenizers 或 accelerate 等辅助库，可额外安装\n",
        "!pip install accelerate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "首先，安装需要的库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math, time, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 尝试导入 matplotlib 用于画图；若不可用则跳过画图功能\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    HAVE_MPL = True\n",
        "except ImportError:\n",
        "    print(\"matplotlib 不可用，图表将被跳过\")\n",
        "    HAVE_MPL = False\n",
        "\n",
        "# 选择运行设备：如果有 GPU 则用 GPU，否则用 CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"使用设备：\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在这个 cell，我们导入 Python 和 PyTorch 的基础依赖，判断是否能画图，并确定计算设备（CPU 或 GPU）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 构造极简解码器 + KV 缓存机制（带注释）\n",
        "下面的代码定义一个非常简化的 Transformer 解码器层和语言模型，并显式管理 KV 缓存。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=128, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads  # 每个 head 的维度\n",
        "        # Q, K, V 三个线性层\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        # 输出映射层\n",
        "        self.o_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, kv_cache: Optional[Dict[str, torch.Tensor]] = None):\n",
        "        \"\"\"\n",
        "        参数：\n",
        "          x: 输入张量形状 [B, T, D]\n",
        "          kv_cache: 如果有历史缓存，则是 dict 包含 \"k\" 和 \"v\"，形状 [B, T_kv, H, Dh]\n",
        "        返回：\n",
        "          out: 输出张量 [B, T, D]\n",
        "          new_cache: 最新的 KV 缓存\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        H, Dh = self.n_heads, self.d_head\n",
        "\n",
        "        # 线性变换，得到 Q, K, V，并 reshape 为 [B, T, H, Dh]\n",
        "        q = self.q_proj(x).view(B, T, H, Dh)\n",
        "        k = self.k_proj(x).view(B, T, H, Dh)\n",
        "        v = self.v_proj(x).view(B, T, H, Dh)\n",
        "\n",
        "        # 如果有历史缓存，就把历史 K, V 拼接到当前 K, V 的前面\n",
        "        if kv_cache is not None and \"k\" in kv_cache:\n",
        "            k = torch.cat([kv_cache[\"k\"], k], dim=1)\n",
        "            v = torch.cat([kv_cache[\"v\"], v], dim=1)\n",
        "\n",
        "        # 构建新的缓存（detach 掉梯度），以供下步 decode 使用\n",
        "        new_cache = {\"k\": k.detach(), \"v\": v.detach()}\n",
        "\n",
        "        # 准备做 attention：reshape 为 [B*H, T, Dh]\n",
        "        q_ = q.transpose(1, 2).contiguous().view(B*H, T, Dh)\n",
        "        k_ = k.transpose(1, 2).contiguous().view(B*H, -1, Dh)\n",
        "        v_ = v.transpose(1, 2).contiguous().view(B*H, -1, Dh)\n",
        "\n",
        "        # 计算 attention 分数（q·k^T / sqrt(Dh)），然后 softmax\n",
        "        attn = torch.bmm(q_, k_.transpose(1, 2)) / math.sqrt(Dh)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        # 用 attention 权重乘 v\n",
        "        out = torch.bmm(attn, v_)\n",
        "\n",
        "        # 还原形状到 [B, T, D]\n",
        "        out = out.view(B, H, T, Dh).transpose(1, 2).contiguous().view(B, T, D)\n",
        "        out = self.o_proj(out)\n",
        "\n",
        "        return out, new_cache\n",
        "\n",
        "class TinyLM(nn.Module):\n",
        "    def __init__(self, vocab_size=2000, d_model=128, n_layers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        # embedding 层，将 token id 映射为向量\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        # 多个 decoder 层组成模型\n",
        "        self.layers = nn.ModuleList([TinyDecoderLayer(d_model, n_heads)\n",
        "                                     for _ in range(n_layers)])\n",
        "        # 最后做线性映射得到 vocab 大小的 logits\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_tokens(self, tokens: torch.Tensor,\n",
        "                       kv_caches: Optional[List[Dict[str, torch.Tensor]]] = None):\n",
        "        \"\"\"\n",
        "        参数：\n",
        "          tokens: [B, T] 的 token id 序列\n",
        "          kv_caches: 每层的历史缓存，形式为 list of dict\n",
        "        返回：\n",
        "          logits: [B, T, vocab_size]\n",
        "          new_caches: 每层新的缓存列表\n",
        "        \"\"\"\n",
        "        x = self.emb(tokens)\n",
        "        new_caches = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            kv = None if kv_caches is None else kv_caches[i]\n",
        "            x, new_kv = layer(x, kv)\n",
        "            new_caches.append(new_kv)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, new_caches\n",
        "\n",
        "# 实例化模型，搬到指定设备，切换为 eval 模式（不启用 dropout 等）\n",
        "model = TinyLM().to(device).eval()\n",
        "print(\"vocab size:\", model.lm_head.out_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这个代码块定义了解码器层 + 模型，并在模型中明确处理历史 KV 缓存拼接，注释清晰标出每一步的目的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def demo_prefill_decode(model, L_prompt=16, decode_steps=5, batch=1):\n",
        "    # 初始时没有任何 KV 缓存\n",
        "    kv_caches = None\n",
        "    # 随机生成一个 prompt 序列，长为 L_prompt\n",
        "    prompt = torch.randint(0, model.emb.num_embeddings,\n",
        "                           (batch, L_prompt), device=device)\n",
        "    # Prefill 阶段：一次性输入整个 prompt\n",
        "    logits, kv_caches = model.forward_tokens(prompt, kv_caches=kv_caches)\n",
        "    # 打印各层缓存中 k 的时间维度长度\n",
        "    print(\"[Prefill] 各层 KV 长度：\", [kv[\"k\"].shape[1] for kv in kv_caches])\n",
        "\n",
        "    # decode 阶段：一步一步地生成 token\n",
        "    last = prompt[:, -1:].clone()\n",
        "    for t in range(decode_steps):\n",
        "        logits, kv_caches = model.forward_tokens(last, kv_caches=kv_caches)\n",
        "        # 从 logits 中取最大得分 token 作为下一个输入\n",
        "        next_tok = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "        last = next_tok\n",
        "        print(f\"[Decode 第 {t+1} 步] 各层 KV 长度：\",\n",
        "              [kv[\"k\"].shape[1] for kv in kv_caches])\n",
        "\n",
        "demo_prefill_decode(model, L_prompt=16, decode_steps=5, batch=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这个 cell 演示了 prefill 和 decode 的完整流程：\n",
        "- Prefill：模型一次性“读入”整个 prompt，并生成初始 KV 缓存\n",
        "- Decode：每一步输入上一步生成的 token，继续使用缓存并追加新的 KV\n",
        "- 注释里解释每一步的操作意图，例如为何拼接缓存、为何取最大 token 等"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 连续批处理（Continuous Batching）仿真（带注释）\n",
        "在真实系统里，多条请求同时进入时，我们希望把 prefill 与 decode 混合调度以提升资源利用率。\n",
        "\n",
        "启动 MII 服务"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mii\n",
        "\n",
        "# 选择一个中小模型便于测试\n",
        "mii.serve(\n",
        "    \"facebook/opt-125m\",\n",
        "    profile_model_time=True,   # 开启各阶段耗时日志\n",
        "    max_length=1024,\n",
        "    tensor_parallel=1          # 如果你有多卡可设为 2、4…\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "客户端错峰请求演示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import threading\n",
        "import mii\n",
        "\n",
        "client = mii.client(\"facebook/opt-125m\")\n",
        "\n",
        "prompts = [\n",
        "    \"A: \" + \"hello \" * 400 + \"\\nQ: Summarize in one sentence.\",\n",
        "    \"Explain the significance of attention masks in Transformers.\",\n",
        "    \"Write a haiku about GPUs.\"\n",
        "]\n",
        "\n",
        "def run_req(prompt, delay, name):\n",
        "    time.sleep(delay)\n",
        "    print(f\"[{name}] send @+{delay:.1f}s\")\n",
        "    resp = client([prompt], max_new_tokens=64, stream=False)\n",
        "    print(f\"[{name}] done, text[:120]= {resp[0].generated_text[:120]!r}\")\n",
        "\n",
        "threads = [\n",
        "    threading.Thread(target=run_req, args=(prompts[0], 0.0,   \"R1-long\")),\n",
        "    threading.Thread(target=run_req, args=(prompts[1], 0.5,   \"R2-mid\")),\n",
        "    threading.Thread(target=run_req, args=(prompts[2], 1.2,   \"R3-short\")),\n",
        "]\n",
        "\n",
        "for t in threads: t.start()\n",
        "for t in threads: t.join()\n",
        "\n",
        "print(\"All done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "日志解析 + 绘图展示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 假设你把 MII 服务日志重定向到 “mii_server.log”\n",
        "# 例如启动服务时用： python serve.py > mii_server.log 2>&1 &\n",
        "\n",
        "logfile = \"./fastgen_out.log\"\n",
        "\n",
        "# 正则要根据日志格式微调，下面是一个示例：\n",
        "# 假设每 step 日志有这样一行：\n",
        "#   “[Step=10] prefill_tokens=123 decode_tokens=456 …”\n",
        "pattern = re.compile(r\"\\[Step=(\\d+)\\]\\s+prefill_tokens=(\\d+)\\s+decode_tokens=(\\d+)\")\n",
        "\n",
        "steps = []\n",
        "prefill_tokens = []\n",
        "decode_tokens = []\n",
        "\n",
        "with open(logfile, 'r') as f:\n",
        "    for line in f:\n",
        "        m = pattern.search(line)\n",
        "        if m:\n",
        "            step = int(m.group(1))\n",
        "            p = int(m.group(2))\n",
        "            d = int(m.group(3))\n",
        "            steps.append(step)\n",
        "            prefill_tokens.append(p)\n",
        "            decode_tokens.append(d)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(steps, prefill_tokens, label=\"prefill tokens / step\")\n",
        "plt.plot(steps, decode_tokens, label=\"decode tokens / step\")\n",
        "plt.xlabel(\"step index\")\n",
        "plt.ylabel(\"tokens\")\n",
        "plt.title(\"连续批处理中每 step 的 prefill / decode 分布\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这一段代码仿真了“多请求同时到来”的场景：\n",
        "- 定义 Request 对象保存每条请求的状态\n",
        "- `step_continuous_batch` 按照三种策略分配预算\n",
        "- `simulate_continuous_batch` 跑一轮并输出各策略结果\n",
        "- 注释在代码中解释每一步为何这么做"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. chunked prefill（分块预填）仿真 + 注释\n",
        "该部分展示切块预填如何在性能与资源占用之间做折中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def chunked_prefill_cost(model, L_prompt=2048, chunk_size=256, sleep_per_chunk_ms=5):\n",
        "    # 计算需要多少块\n",
        "    steps = math.ceil(L_prompt / chunk_size)\n",
        "    # 随机生成一块输入作为真实前向\n",
        "    dummy = torch.randint(0, model.emb.num_embeddings,\n",
        "                          (1, min(chunk_size, L_prompt)), device=device)\n",
        "    st = time.time()\n",
        "    # 对第一块做真实前向 + 缓存计算\n",
        "    logits, kv = model.forward_tokens(dummy, kv_caches=None)\n",
        "    # 对剩余块，只做模拟开销（sleep），不真正计算\n",
        "    for _ in range(steps - 1):\n",
        "        if sleep_per_chunk_ms > 0:\n",
        "            time.sleep(sleep_per_chunk_ms / 1000.0)\n",
        "    et = time.time()\n",
        "    return steps, et - st\n",
        "\n",
        "for cs in [64, 128, 256, 512]:\n",
        "    s, t = chunked_prefill_cost(model, L_prompt=2048,\n",
        "                                 chunk_size=cs, sleep_per_chunk_ms=5)\n",
        "    print(f\"chunk_size = {cs:>4}, chunk 数 = {s:>3}, approx time = {t:.3f} 秒\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "此 cell：\n",
        "- 计算把 prompt 切块后需要多少块，以及模拟整体耗时\n",
        "- 我们只对第一块做真实前向，其他块用 sleep 模拟开销，突出块数 / 大小对性能的影响\n",
        "- 注释里标明为什么这样设计，以及每一步的目的"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dynamic SplitFuse 调度仿真 + 注释\n",
        "这个部分是整个 notebook 的重头戏：仿真 Split + Fuse 调度策略。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Piece:\n",
        "    rid: int\n",
        "    is_last: bool\n",
        "    size: int\n",
        "\n",
        "def split_prompts(Ls: List[int], max_piece: int) -> List[Piece]:\n",
        "    \"\"\"\n",
        "    把每个 prompt 拆成若干块 (piece)，每块大小 ≤ max_piece\n",
        "    标记最后一块 is_last = True，以便调度允许 decode\n",
        "    \"\"\"\n",
        "    pieces = []\n",
        "    for rid, L in enumerate(Ls):\n",
        "        full, rem = divmod(L, max_piece)\n",
        "        for _ in range(full):\n",
        "            pieces.append(Piece(rid, False, max_piece))\n",
        "        if rem > 0:\n",
        "            pieces.append(Piece(rid, True, rem))\n",
        "        elif full > 0:\n",
        "            # 如果刚好整除，则最后那块也标记为最后一块\n",
        "            pieces[-1] = Piece(rid, True, max_piece)\n",
        "        else:\n",
        "            # prompt 长度为 0 的特殊情况，也标记为最后一块\n",
        "            pieces.append(Piece(rid, True, 0))\n",
        "    return pieces\n",
        "\n",
        "def dynamic_split_fuse(L_prompts: List[int], L_generate: List[int],\n",
        "                       budget: int, max_piece: int):\n",
        "    \"\"\"\n",
        "    调度仿真：\n",
        "    每 step 有固定 token 预算 budget。\n",
        "    - Fuse：尽量把拆好的 prefill 块填满预算\n",
        "    - Split：长 prompt 分块逐步 prefill\n",
        "    - 只有属于最后一块的请求，允许用剩下预算做 decode\n",
        "    返回三条历史记录：prefill_hist, decode_hist, piece_hist\n",
        "    \"\"\"\n",
        "    from collections import deque\n",
        "    pieces = split_prompts(L_prompts, max_piece)\n",
        "    queue = deque(pieces)\n",
        "    remain_gen = {i: g for i, g in enumerate(L_generate)}\n",
        "    prefill_hist = []\n",
        "    decode_hist = []\n",
        "    piece_hist = []\n",
        "\n",
        "    while len(queue) > 0 or any(v > 0 for v in remain_gen.values()):\n",
        "        budget_left = budget\n",
        "        used_pieces = []\n",
        "        step_prefill = 0\n",
        "        step_decode = 0\n",
        "        last_piece_reqs = set()\n",
        "\n",
        "        # Fuse 阶段：尽可能把多个 prefill 块塞满这个 step\n",
        "        temp = deque()\n",
        "        while budget_left > 0 and queue:\n",
        "            p = queue.popleft()\n",
        "            if p.size <= budget_left:\n",
        "                used_pieces.append(p)\n",
        "                budget_left -= p.size\n",
        "                step_prefill += p.size\n",
        "                if p.is_last:\n",
        "                    last_piece_reqs.add(p.rid)\n",
        "            else:\n",
        "                temp.appendleft(p)\n",
        "                break\n",
        "        queue = temp + queue\n",
        "\n",
        "        # 对于那些最后一块对应的请求，用剩余预算做 decode\n",
        "        for rid in list(last_piece_reqs):\n",
        "            if budget_left == 0:\n",
        "                break\n",
        "            need = remain_gen[rid]\n",
        "            if need <= 0:\n",
        "                continue\n",
        "            use = min(need, budget_left)\n",
        "            remain_gen[rid] -= use\n",
        "            budget_left -= use\n",
        "            step_decode += use\n",
        "\n",
        "        prefill_hist.append(step_prefill)\n",
        "        decode_hist.append(step_decode)\n",
        "        piece_hist.append(len(used_pieces))\n",
        "        # 如果本 step 完全没做任何事，就跳出避免死循环\n",
        "        if step_prefill == 0 and step_decode == 0:\n",
        "            break\n",
        "\n",
        "    return prefill_hist, decode_hist, piece_hist\n",
        "\n",
        "# 示例：用几条 prompt 长度差异大的请求来仿真\n",
        "L_prompts = [1536, 128, 512, 4096, 64]\n",
        "L_generate = [64, 64, 64, 64, 64]\n",
        "budget = 1024\n",
        "max_piece = 512\n",
        "\n",
        "prefill_hist, decode_hist, piece_hist = dynamic_split_fuse(\n",
        "    L_prompts, L_generate, budget, max_piece\n",
        ")\n",
        "print(\"总步数 =\", len(prefill_hist),\n",
        "      \"总 prefill =\", sum(prefill_hist),\n",
        "      \"总 decode =\", sum(decode_hist))\n",
        "\n",
        "if HAVE_MPL:\n",
        "    xs = list(range(len(prefill_hist)))\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(xs, prefill_hist, label=\"prefill\")\n",
        "    plt.plot(xs, decode_hist, label=\"decode\")\n",
        "    plt.plot(xs, piece_hist, label=\"#pieces used\")\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"tokens / pieces\")\n",
        "    plt.title(\"Dynamic SplitFuse 调度仿真\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这部分代码在做的事情：\n",
        "- `split_prompts` 把 prompt 拆为块（piece），并标记最后一块\n",
        "- `dynamic_split_fuse` 在每 step 内执行 Fuse + Split + decode 调度\n",
        "- 保证拆、融合与 decode 共存，使得每一步 workload 较为平稳\n",
        "- 注释中解释为何允许 decode 只在最后一块、为何先 Fuse 等细节"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. DeepSpeed-MII / vLLM 接口演示（带注释）\n",
        "在你理解 toy 模型后，这里给出如何在实际库中调用 FastGen / chunked prefill 的示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DeepSpeed-MII 示例：快速上手 pipeline 接口（需安装 deepspeed-mii）\n",
        "from mii import pipeline\n",
        "\n",
        "# 以 Mistral-7B 模型为例构造 pipeline\n",
        "pipe = pipeline(\"mistralai/Mistral-7B-v0.1\")\n",
        "# 给它两个 prompt，生成最多 64 个新 token\n",
        "outputs = pipe([\"你好，今天天气如何？\", \"请写一首诗\"], max_new_tokens=64)\n",
        "print(\"DeepSpeed-MII 输出：\", outputs)\n",
        "\n",
        "# vLLM 示例：启用 chunked prefill 功能（需安装 vllm）\n",
        "from vllm import LLM\n",
        "# 指定模型 + 开启 enable_chunked_prefill\n",
        "llm = LLM(model=\"meta-llama/Llama-2-7b-hf\", enable_chunked_prefill=True)\n",
        "outs = llm.generate([\"今天天气怎么样？\", \"写一首诗\"], max_tokens=64)\n",
        "print(\"vLLM 输出：\", outs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这个 cell 演示如何使用真实库：\n",
        "- 用 DeepSpeed-MII 的 `pipeline` 接口启动推理服务\n",
        "- 用 vLLM 启用 `enable_chunked_prefill` 特性进行生成\n",
        "- 注释中解释每一步调用的意义，和我们前面 toy 模型对比"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
